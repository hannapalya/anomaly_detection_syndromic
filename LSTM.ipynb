{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMG/EeDlLZCvN2RXUVmBs9M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannapalya/anomaly_detection_syndromic/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuYnY9zfNGAC",
        "outputId": "fb4a9ab2-51fa-4dec-dd22-4af88ac4a0e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "\n",
            "=== Signal 1 (LSTM AE; tail-only metrics) ===\n",
            "  Saved indices: val=200 → lstm_val_indices_sig1.csv,  test=200 → lstm_test_indices_sig1.csv\n",
            "  Train windows: 1302600 from first 6 years\n",
            "  Val windows (tail): 66000  positives: 3366 (5.1%)\n",
            "  Test windows (tail): 66000\n",
            "Epoch 1/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 31ms/step - loss: 156.6994 - mse: 97330.2344 - val_loss: 74.5295 - val_mse: 12205.1758 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 31ms/step - loss: 42.2211 - mse: 5972.9272 - val_loss: 59.1011 - val_mse: 7628.2207 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 31ms/step - loss: 39.4475 - mse: 4800.7227 - val_loss: 85.9164 - val_mse: 30516.6133 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 31ms/step - loss: 38.4455 - mse: 4481.5835 - val_loss: 33.8840 - val_mse: 3483.3225 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 31ms/step - loss: 36.6921 - mse: 3759.5613 - val_loss: 87.6421 - val_mse: 17340.5605 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 31ms/step - loss: 36.1072 - mse: 3486.6265 - val_loss: 31.7599 - val_mse: 3198.7192 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 31ms/step - loss: 35.3483 - mse: 3253.4807 - val_loss: 31.9458 - val_mse: 3106.1370 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 32ms/step - loss: 34.7022 - mse: 3081.9458 - val_loss: 31.1450 - val_mse: 2794.9004 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 32ms/step - loss: 34.8238 - mse: 3159.4260 - val_loss: 33.5096 - val_mse: 3375.6880 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 32ms/step - loss: 34.5876 - mse: 3039.3831 - val_loss: 32.8681 - val_mse: 3928.6794 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 32ms/step - loss: 35.0961 - mse: 3219.0903 - val_loss: 38.9881 - val_mse: 4408.4092 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 32ms/step - loss: 34.3800 - mse: 3089.9758 - val_loss: 44.4294 - val_mse: 4379.8877 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 32ms/step - loss: 33.8008 - mse: 2863.0291 - val_loss: 31.8885 - val_mse: 2858.6260 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 32ms/step - loss: 33.2132 - mse: 2704.8123 - val_loss: 28.9639 - val_mse: 2576.9783 - learning_rate: 5.0000e-04\n",
            "Epoch 15/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 32ms/step - loss: 32.3700 - mse: 2575.5159 - val_loss: 29.0402 - val_mse: 2408.3853 - learning_rate: 5.0000e-04\n",
            "Epoch 16/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 32ms/step - loss: 32.6733 - mse: 2594.6543 - val_loss: 29.1750 - val_mse: 2253.0867 - learning_rate: 5.0000e-04\n",
            "Epoch 17/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 32ms/step - loss: 31.6467 - mse: 2338.9185 - val_loss: 27.4233 - val_mse: 2084.9841 - learning_rate: 5.0000e-04\n",
            "Epoch 18/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 32ms/step - loss: 32.0062 - mse: 2403.5742 - val_loss: 269.6967 - val_mse: 183038.7500 - learning_rate: 5.0000e-04\n",
            "Epoch 19/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 32ms/step - loss: 31.6972 - mse: 2302.7207 - val_loss: 27.4797 - val_mse: 2080.9683 - learning_rate: 5.0000e-04\n",
            "Epoch 20/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 32ms/step - loss: 30.6971 - mse: 2132.0164 - val_loss: 29.6839 - val_mse: 2495.6445 - learning_rate: 5.0000e-04\n",
            "Epoch 21/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 32ms/step - loss: 31.2695 - mse: 2256.1980 - val_loss: 31.7900 - val_mse: 2581.1318 - learning_rate: 5.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 32ms/step - loss: 31.1212 - mse: 2204.4419 - val_loss: 53.0597 - val_mse: 6629.3691 - learning_rate: 5.0000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 32ms/step - loss: 30.2943 - mse: 2068.8789 - val_loss: 26.8910 - val_mse: 2080.9155 - learning_rate: 2.5000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 32ms/step - loss: 29.9321 - mse: 1994.6379 - val_loss: 26.6847 - val_mse: 1976.3092 - learning_rate: 2.5000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 32ms/step - loss: 29.8548 - mse: 1988.6952 - val_loss: 27.3204 - val_mse: 2357.4475 - learning_rate: 2.5000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 32ms/step - loss: 30.0205 - mse: 2038.3501 - val_loss: 26.5747 - val_mse: 1972.1068 - learning_rate: 2.5000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 32ms/step - loss: 29.9486 - mse: 2004.7408 - val_loss: 27.3376 - val_mse: 2152.7168 - learning_rate: 2.5000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 32ms/step - loss: 29.6719 - mse: 1962.3315 - val_loss: 27.4669 - val_mse: 2338.0647 - learning_rate: 2.5000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 32ms/step - loss: 29.5674 - mse: 1958.4736 - val_loss: 27.6062 - val_mse: 2306.6663 - learning_rate: 2.5000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 32ms/step - loss: 29.9704 - mse: 2054.9209 - val_loss: 31.5459 - val_mse: 2600.0959 - learning_rate: 2.5000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 32ms/step - loss: 30.0339 - mse: 2032.6626 - val_loss: 26.2659 - val_mse: 2108.2239 - learning_rate: 2.5000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 32ms/step - loss: 29.4175 - mse: 1933.7942 - val_loss: 26.6083 - val_mse: 1903.6603 - learning_rate: 2.5000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 32ms/step - loss: 29.4952 - mse: 1937.2812 - val_loss: 26.8347 - val_mse: 2507.6438 - learning_rate: 2.5000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 32ms/step - loss: 29.5465 - mse: 1939.3793 - val_loss: 26.4781 - val_mse: 2317.4417 - learning_rate: 2.5000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 32ms/step - loss: 29.5941 - mse: 2008.3713 - val_loss: 26.3433 - val_mse: 2360.0891 - learning_rate: 2.5000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 32ms/step - loss: 29.8999 - mse: 2041.7344 - val_loss: 26.6702 - val_mse: 2354.4607 - learning_rate: 2.5000e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 32ms/step - loss: 29.7739 - mse: 2002.4307 - val_loss: 25.9556 - val_mse: 2150.6458 - learning_rate: 1.2500e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 32ms/step - loss: 29.2517 - mse: 1906.7529 - val_loss: 26.1544 - val_mse: 2273.1384 - learning_rate: 1.2500e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 32ms/step - loss: 29.1004 - mse: 1878.2164 - val_loss: 26.0002 - val_mse: 2165.6150 - learning_rate: 1.2500e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 32ms/step - loss: 29.0757 - mse: 1863.5714 - val_loss: 25.8940 - val_mse: 2330.6895 - learning_rate: 1.2500e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 32ms/step - loss: 29.2463 - mse: 1964.4390 - val_loss: 25.7301 - val_mse: 2131.8176 - learning_rate: 1.2500e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 32ms/step - loss: 29.2065 - mse: 1888.3475 - val_loss: 26.1831 - val_mse: 2519.8113 - learning_rate: 1.2500e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 32ms/step - loss: 29.0557 - mse: 1873.8226 - val_loss: 26.0613 - val_mse: 2165.8433 - learning_rate: 1.2500e-04\n",
            "Epoch 44/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 32ms/step - loss: 29.0919 - mse: 1857.4913 - val_loss: 28.7208 - val_mse: 2525.6799 - learning_rate: 1.2500e-04\n",
            "Epoch 45/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 32ms/step - loss: 28.8995 - mse: 1830.9847 - val_loss: 25.7210 - val_mse: 2171.7312 - learning_rate: 1.2500e-04\n",
            "Epoch 46/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 32ms/step - loss: 28.8700 - mse: 1836.5267 - val_loss: 25.8233 - val_mse: 2346.5615 - learning_rate: 1.2500e-04\n",
            "Epoch 47/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 32ms/step - loss: 28.7194 - mse: 1798.2208 - val_loss: 27.2103 - val_mse: 2725.3157 - learning_rate: 1.2500e-04\n",
            "Epoch 48/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 32ms/step - loss: 28.6858 - mse: 1802.8552 - val_loss: 25.2577 - val_mse: 2229.4741 - learning_rate: 1.2500e-04\n",
            "Epoch 49/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 32ms/step - loss: 28.9318 - mse: 1836.3630 - val_loss: 26.1737 - val_mse: 2254.6182 - learning_rate: 1.2500e-04\n",
            "Epoch 50/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 32ms/step - loss: 28.6871 - mse: 1779.6643 - val_loss: 27.0934 - val_mse: 2428.5674 - learning_rate: 1.2500e-04\n",
            "Epoch 51/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 32ms/step - loss: 28.6913 - mse: 1829.2850 - val_loss: 25.3305 - val_mse: 2014.1979 - learning_rate: 1.2500e-04\n",
            "Epoch 52/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 32ms/step - loss: 28.8919 - mse: 1827.5493 - val_loss: 25.9797 - val_mse: 2384.6299 - learning_rate: 1.2500e-04\n",
            "Epoch 53/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 32ms/step - loss: 28.9592 - mse: 1845.0719 - val_loss: 25.2318 - val_mse: 2194.9890 - learning_rate: 1.2500e-04\n",
            "Epoch 54/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 32ms/step - loss: 28.6224 - mse: 1807.4825 - val_loss: 25.5838 - val_mse: 2267.9358 - learning_rate: 1.2500e-04\n",
            "Epoch 55/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 32ms/step - loss: 28.7312 - mse: 1828.2739 - val_loss: 25.2034 - val_mse: 2288.0483 - learning_rate: 1.2500e-04\n",
            "Epoch 56/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 32ms/step - loss: 28.7844 - mse: 1793.5322 - val_loss: 24.6267 - val_mse: 1921.7435 - learning_rate: 1.2500e-04\n",
            "Epoch 57/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 32ms/step - loss: 28.6580 - mse: 1799.3641 - val_loss: 26.0053 - val_mse: 2010.4747 - learning_rate: 1.2500e-04\n",
            "Epoch 58/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 32ms/step - loss: 28.5968 - mse: 1762.0675 - val_loss: 26.3118 - val_mse: 2190.8479 - learning_rate: 1.2500e-04\n",
            "Epoch 59/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 32ms/step - loss: 28.4858 - mse: 1778.2948 - val_loss: 25.6135 - val_mse: 2143.8704 - learning_rate: 1.2500e-04\n",
            "Epoch 60/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 32ms/step - loss: 28.5692 - mse: 1753.7874 - val_loss: 24.9207 - val_mse: 2039.5026 - learning_rate: 1.2500e-04\n",
            "  Chosen mix: MSE=0.10, MAX=0.90 | threshold=379.613495 | val Sens=0.796, Spec=0.980\n",
            "  TAIL-ONLY → Sens=0.760, Spec=0.980, FPR=0.020, POD=0.990, Tim=0.107\n",
            "\n",
            "=== Signal 2 (LSTM AE; tail-only metrics) ===\n",
            "  Saved indices: val=200 → lstm_val_indices_sig2.csv,  test=200 → lstm_test_indices_sig2.csv\n",
            "  Train windows: 1302600 from first 6 years\n",
            "  Val windows (tail): 66000  positives: 1770 (2.7%)\n",
            "  Test windows (tail): 66000\n",
            "Epoch 1/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 33ms/step - loss: 1.4860 - mse: 12.4928 - val_loss: 1.0209 - val_mse: 6.7806 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 33ms/step - loss: 0.9497 - mse: 4.4219 - val_loss: 0.9797 - val_mse: 6.7263 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 33ms/step - loss: 0.8912 - mse: 3.9478 - val_loss: 0.8892 - val_mse: 5.8196 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 32ms/step - loss: 0.8606 - mse: 3.7327 - val_loss: 0.8855 - val_mse: 5.9556 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 29ms/step - loss: 0.8479 - mse: 3.6360 - val_loss: 0.8792 - val_mse: 5.9052 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 29ms/step - loss: 0.8317 - mse: 3.5166 - val_loss: 0.8621 - val_mse: 5.8605 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 29ms/step - loss: 0.8234 - mse: 3.4793 - val_loss: 0.8557 - val_mse: 5.8782 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 29ms/step - loss: 0.8117 - mse: 3.3981 - val_loss: 0.8469 - val_mse: 5.6213 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 0.8096 - mse: 3.3882 - val_loss: 0.8876 - val_mse: 5.8581 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 30ms/step - loss: 0.8050 - mse: 3.3514 - val_loss: 0.9018 - val_mse: 6.1565 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 30ms/step - loss: 0.8008 - mse: 3.3393 - val_loss: 0.8647 - val_mse: 5.7919 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 0.7931 - mse: 3.2874 - val_loss: 0.8289 - val_mse: 5.5443 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 30ms/step - loss: 0.7913 - mse: 3.2818 - val_loss: 0.8530 - val_mse: 5.6433 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 30ms/step - loss: 0.7895 - mse: 3.2790 - val_loss: 0.8586 - val_mse: 5.7000 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 29ms/step - loss: 0.7877 - mse: 3.2672 - val_loss: 0.8566 - val_mse: 5.7015 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 30ms/step - loss: 0.7827 - mse: 3.2309 - val_loss: 0.8377 - val_mse: 5.5664 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 30ms/step - loss: 0.7831 - mse: 3.2310 - val_loss: 0.8230 - val_mse: 5.6247 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 30ms/step - loss: 0.7841 - mse: 3.2358 - val_loss: 0.8661 - val_mse: 5.6571 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 30ms/step - loss: 0.7798 - mse: 3.2158 - val_loss: 0.8772 - val_mse: 5.6598 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 0.7763 - mse: 3.2013 - val_loss: 0.8683 - val_mse: 5.7004 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 30ms/step - loss: 0.7762 - mse: 3.2008 - val_loss: 0.8336 - val_mse: 5.4335 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 0.7807 - mse: 3.2338 - val_loss: 0.8395 - val_mse: 5.5060 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 0.7600 - mse: 3.1077 - val_loss: 0.8170 - val_mse: 5.3663 - learning_rate: 5.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 31ms/step - loss: 0.7555 - mse: 3.0927 - val_loss: 0.8143 - val_mse: 5.3654 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 0.7542 - mse: 3.0834 - val_loss: 0.8220 - val_mse: 5.4107 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 0.7528 - mse: 3.0746 - val_loss: 0.8194 - val_mse: 5.3420 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 0.7542 - mse: 3.0840 - val_loss: 0.8190 - val_mse: 5.4280 - learning_rate: 5.0000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 31ms/step - loss: 0.7515 - mse: 3.0726 - val_loss: 0.8127 - val_mse: 5.2582 - learning_rate: 5.0000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 0.7497 - mse: 3.0611 - val_loss: 0.8100 - val_mse: 5.3309 - learning_rate: 5.0000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 0.7484 - mse: 3.0508 - val_loss: 0.8204 - val_mse: 5.3581 - learning_rate: 5.0000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 0.7500 - mse: 3.0616 - val_loss: 0.8187 - val_mse: 5.3660 - learning_rate: 5.0000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 0.7498 - mse: 3.0609 - val_loss: 0.8208 - val_mse: 5.3732 - learning_rate: 5.0000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 0.7486 - mse: 3.0511 - val_loss: 0.8216 - val_mse: 5.3517 - learning_rate: 5.0000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 0.7467 - mse: 3.0471 - val_loss: 0.8101 - val_mse: 5.3218 - learning_rate: 5.0000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 0.7401 - mse: 3.0098 - val_loss: 0.8061 - val_mse: 5.2767 - learning_rate: 2.5000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 0.7384 - mse: 3.0082 - val_loss: 0.8247 - val_mse: 5.3683 - learning_rate: 2.5000e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 0.7380 - mse: 3.0034 - val_loss: 0.8181 - val_mse: 5.3851 - learning_rate: 2.5000e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 0.7365 - mse: 2.9964 - val_loss: 0.8087 - val_mse: 5.2941 - learning_rate: 2.5000e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 0.7368 - mse: 2.9977 - val_loss: 0.8057 - val_mse: 5.3322 - learning_rate: 2.5000e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 0.7351 - mse: 2.9924 - val_loss: 0.8204 - val_mse: 5.4963 - learning_rate: 2.5000e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 0.7356 - mse: 2.9882 - val_loss: 0.8287 - val_mse: 5.5449 - learning_rate: 2.5000e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 0.7353 - mse: 2.9888 - val_loss: 0.8089 - val_mse: 5.2783 - learning_rate: 2.5000e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 29ms/step - loss: 0.7339 - mse: 2.9834 - val_loss: 0.8190 - val_mse: 5.3349 - learning_rate: 2.5000e-04\n",
            "Epoch 44/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 29ms/step - loss: 0.7355 - mse: 2.9878 - val_loss: 0.8107 - val_mse: 5.2685 - learning_rate: 2.5000e-04\n",
            "Epoch 45/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 29ms/step - loss: 0.7317 - mse: 2.9712 - val_loss: 0.8089 - val_mse: 5.2849 - learning_rate: 1.2500e-04\n",
            "Epoch 46/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 29ms/step - loss: 0.7308 - mse: 2.9622 - val_loss: 0.8108 - val_mse: 5.3363 - learning_rate: 1.2500e-04\n",
            "  Chosen mix: MSE=0.60, MAX=0.40 | threshold=12.069081 | val Sens=0.534, Spec=0.974\n",
            "  TAIL-ONLY → Sens=0.543, Spec=0.977, FPR=0.023, POD=0.520, Tim=0.551\n",
            "\n",
            "=== Signal 3 (LSTM AE; tail-only metrics) ===\n",
            "  Saved indices: val=200 → lstm_val_indices_sig3.csv,  test=200 → lstm_test_indices_sig3.csv\n",
            "  Train windows: 1302600 from first 6 years\n",
            "  Val windows (tail): 66000  positives: 2973 (4.5%)\n",
            "  Test windows (tail): 66000\n",
            "Epoch 1/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 30ms/step - loss: 66.1778 - mse: 15327.0254 - val_loss: 19.6455 - val_mse: 1667.9425 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 29ms/step - loss: 21.4515 - mse: 1778.5166 - val_loss: 119.2315 - val_mse: 25547.6387 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 29ms/step - loss: 19.3133 - mse: 1398.2866 - val_loss: 109.4345 - val_mse: 17779.8008 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 30ms/step - loss: 18.7322 - mse: 1306.2104 - val_loss: 47.3957 - val_mse: 7849.7681 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 30ms/step - loss: 18.7114 - mse: 1314.4244 - val_loss: 17.4574 - val_mse: 1281.4569 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 30ms/step - loss: 17.7008 - mse: 1115.0492 - val_loss: 17.2824 - val_mse: 1206.0228 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 30ms/step - loss: 17.4030 - mse: 1053.0635 - val_loss: 16.2635 - val_mse: 983.7677 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 30ms/step - loss: 16.4308 - mse: 844.3383 - val_loss: 15.4032 - val_mse: 801.8844 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 16.0667 - mse: 772.0936 - val_loss: 15.2864 - val_mse: 774.7689 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 29ms/step - loss: 15.6714 - mse: 699.0915 - val_loss: 15.3465 - val_mse: 766.2573 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 15.5712 - mse: 692.4338 - val_loss: 15.4337 - val_mse: 812.5688 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 30ms/step - loss: 15.4512 - mse: 675.4726 - val_loss: 15.3938 - val_mse: 812.5861 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 29ms/step - loss: 15.3878 - mse: 671.2245 - val_loss: 16.9830 - val_mse: 1168.1682 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 30ms/step - loss: 15.3956 - mse: 680.5953 - val_loss: 15.3123 - val_mse: 768.2730 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 30ms/step - loss: 15.4218 - mse: 683.7075 - val_loss: 15.0481 - val_mse: 778.5118 - learning_rate: 5.0000e-04\n",
            "Epoch 16/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 30ms/step - loss: 14.9280 - mse: 598.6913 - val_loss: 14.9381 - val_mse: 769.0936 - learning_rate: 5.0000e-04\n",
            "Epoch 17/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 29ms/step - loss: 14.8260 - mse: 587.0297 - val_loss: 15.0768 - val_mse: 772.9706 - learning_rate: 5.0000e-04\n",
            "Epoch 18/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 30ms/step - loss: 14.7835 - mse: 589.7935 - val_loss: 14.7214 - val_mse: 698.2063 - learning_rate: 5.0000e-04\n",
            "Epoch 19/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 29ms/step - loss: 14.7595 - mse: 583.8887 - val_loss: 14.7347 - val_mse: 775.3447 - learning_rate: 5.0000e-04\n",
            "Epoch 20/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 29ms/step - loss: 14.6735 - mse: 579.8746 - val_loss: 14.5719 - val_mse: 709.8882 - learning_rate: 5.0000e-04\n",
            "Epoch 21/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 29ms/step - loss: 14.6218 - mse: 581.4743 - val_loss: 14.6022 - val_mse: 774.7687 - learning_rate: 5.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 29ms/step - loss: 14.4893 - mse: 572.8092 - val_loss: 14.5267 - val_mse: 766.9042 - learning_rate: 5.0000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 29ms/step - loss: 14.4419 - mse: 565.2456 - val_loss: 14.4038 - val_mse: 767.1173 - learning_rate: 5.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 29ms/step - loss: 14.4236 - mse: 569.7297 - val_loss: 14.5609 - val_mse: 690.1865 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 29ms/step - loss: 14.8780 - mse: 619.6119 - val_loss: 14.6265 - val_mse: 766.5340 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 29ms/step - loss: 14.4625 - mse: 568.1188 - val_loss: 14.2788 - val_mse: 709.1443 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 29ms/step - loss: 14.4442 - mse: 566.7703 - val_loss: 14.4237 - val_mse: 734.5873 - learning_rate: 5.0000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 29ms/step - loss: 14.4643 - mse: 565.0184 - val_loss: 14.2251 - val_mse: 712.9240 - learning_rate: 5.0000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 29ms/step - loss: 14.3147 - mse: 559.9479 - val_loss: 14.3161 - val_mse: 753.2181 - learning_rate: 5.0000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 29ms/step - loss: 14.3292 - mse: 561.9238 - val_loss: 15.1017 - val_mse: 715.5735 - learning_rate: 5.0000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 29ms/step - loss: 14.3210 - mse: 561.9415 - val_loss: 14.2908 - val_mse: 775.4657 - learning_rate: 5.0000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 29ms/step - loss: 14.3711 - mse: 577.6718 - val_loss: 14.2581 - val_mse: 765.3787 - learning_rate: 5.0000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 30ms/step - loss: 14.2820 - mse: 570.3322 - val_loss: 15.2981 - val_mse: 760.7104 - learning_rate: 5.0000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 29ms/step - loss: 14.9281 - mse: 622.2144 - val_loss: 14.3051 - val_mse: 770.3922 - learning_rate: 2.5000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 29ms/step - loss: 14.1868 - mse: 550.2840 - val_loss: 14.4926 - val_mse: 787.1759 - learning_rate: 2.5000e-04\n",
            "  Chosen mix: MSE=1.00, MAX=0.00 | threshold=3166.978760 | val Sens=0.702, Spec=0.970\n",
            "  TAIL-ONLY → Sens=0.656, Spec=0.972, FPR=0.028, POD=0.975, Tim=0.210\n",
            "\n",
            "=== Signal 4 (LSTM AE; tail-only metrics) ===\n",
            "  Saved indices: val=200 → lstm_val_indices_sig4.csv,  test=200 → lstm_test_indices_sig4.csv\n",
            "  Train windows: 1302600 from first 6 years\n",
            "  Val windows (tail): 66000  positives: 2124 (3.2%)\n",
            "  Test windows (tail): 66000\n",
            "Epoch 1/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 29ms/step - loss: 2.0671 - mse: 12.1138 - val_loss: 1.5737 - val_mse: 11.0720 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 29ms/step - loss: 1.4599 - mse: 6.1090 - val_loss: 1.5843 - val_mse: 11.5371 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 30ms/step - loss: 1.4108 - mse: 5.8475 - val_loss: 1.5137 - val_mse: 11.5608 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 29ms/step - loss: 1.3932 - mse: 5.7895 - val_loss: 1.5744 - val_mse: 11.6906 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 29ms/step - loss: 1.3761 - mse: 5.6872 - val_loss: 1.4846 - val_mse: 11.1972 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 29ms/step - loss: 1.3496 - mse: 5.5573 - val_loss: 1.4804 - val_mse: 11.1756 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 29ms/step - loss: 1.3516 - mse: 5.5693 - val_loss: 1.4694 - val_mse: 11.0038 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 29ms/step - loss: 1.3316 - mse: 5.4677 - val_loss: 1.4570 - val_mse: 10.8530 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 29ms/step - loss: 1.3358 - mse: 5.4860 - val_loss: 1.5007 - val_mse: 11.2885 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 29ms/step - loss: 1.3262 - mse: 5.4569 - val_loss: 1.4663 - val_mse: 10.7343 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 29ms/step - loss: 1.3188 - mse: 5.3816 - val_loss: 1.4618 - val_mse: 10.9118 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 30ms/step - loss: 1.3198 - mse: 5.3644 - val_loss: 1.4192 - val_mse: 10.7603 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 30ms/step - loss: 1.2993 - mse: 5.2778 - val_loss: 1.4548 - val_mse: 11.0237 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 30ms/step - loss: 1.2924 - mse: 5.2299 - val_loss: 1.5123 - val_mse: 11.0845 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 1.2903 - mse: 5.2659 - val_loss: 1.4649 - val_mse: 11.2167 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 1.3016 - mse: 5.3377 - val_loss: 1.4292 - val_mse: 10.8077 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 1.2915 - mse: 5.2408 - val_loss: 1.4442 - val_mse: 10.8225 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 1.2655 - mse: 5.0833 - val_loss: 1.4114 - val_mse: 10.7952 - learning_rate: 5.0000e-04\n",
            "Epoch 19/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 1.2577 - mse: 5.0912 - val_loss: 1.4185 - val_mse: 10.7756 - learning_rate: 5.0000e-04\n",
            "Epoch 20/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 31ms/step - loss: 1.2588 - mse: 5.0991 - val_loss: 1.4200 - val_mse: 10.7550 - learning_rate: 5.0000e-04\n",
            "Epoch 21/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 1.2548 - mse: 5.0868 - val_loss: 1.4106 - val_mse: 10.6340 - learning_rate: 5.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 1.2540 - mse: 5.0788 - val_loss: 1.4277 - val_mse: 10.7672 - learning_rate: 5.0000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 1.2553 - mse: 5.0831 - val_loss: 1.4174 - val_mse: 10.7192 - learning_rate: 5.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 1.2603 - mse: 5.1254 - val_loss: 1.4449 - val_mse: 10.8164 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 30ms/step - loss: 1.2587 - mse: 5.1267 - val_loss: 1.4210 - val_mse: 10.8236 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 1.2552 - mse: 5.0857 - val_loss: 1.4352 - val_mse: 10.9265 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 30ms/step - loss: 1.2523 - mse: 5.0828 - val_loss: 1.4096 - val_mse: 10.6603 - learning_rate: 2.5000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 30ms/step - loss: 1.2346 - mse: 4.9957 - val_loss: 1.4023 - val_mse: 10.7700 - learning_rate: 2.5000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 30ms/step - loss: 1.2305 - mse: 4.9995 - val_loss: 1.4071 - val_mse: 10.7515 - learning_rate: 2.5000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 30ms/step - loss: 1.2268 - mse: 4.9667 - val_loss: 1.4028 - val_mse: 10.8049 - learning_rate: 2.5000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 30ms/step - loss: 1.2290 - mse: 4.9962 - val_loss: 1.4086 - val_mse: 10.8148 - learning_rate: 2.5000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 1.2286 - mse: 4.9906 - val_loss: 1.4143 - val_mse: 10.8449 - learning_rate: 2.5000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 30ms/step - loss: 1.2286 - mse: 4.9671 - val_loss: 1.4109 - val_mse: 10.8494 - learning_rate: 2.5000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 30ms/step - loss: 1.2212 - mse: 4.9527 - val_loss: 1.4030 - val_mse: 10.8504 - learning_rate: 1.2500e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 1.2224 - mse: 4.9783 - val_loss: 1.4086 - val_mse: 10.7701 - learning_rate: 1.2500e-04\n",
            "  Chosen mix: MSE=0.80, MAX=0.20 | threshold=19.948345 | val Sens=0.876, Spec=0.977\n",
            "  TAIL-ONLY → Sens=0.861, Spec=0.977, FPR=0.023, POD=0.985, Tim=0.134\n",
            "\n",
            "=== Signal 5 (LSTM AE; tail-only metrics) ===\n",
            "  Saved indices: val=200 → lstm_val_indices_sig5.csv,  test=200 → lstm_test_indices_sig5.csv\n",
            "  Train windows: 1302600 from first 6 years\n",
            "  Val windows (tail): 66000  positives: 3219 (4.9%)\n",
            "  Test windows (tail): 66000\n",
            "Epoch 1/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 30ms/step - loss: 35.3700 - mse: 7704.5347 - val_loss: 10.6970 - val_mse: 981.0691 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 30ms/step - loss: 16.7405 - mse: 1847.3580 - val_loss: 24.9022 - val_mse: 2871.5093 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 15.2952 - mse: 1541.4252 - val_loss: 7.9543 - val_mse: 508.2411 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 13.6458 - mse: 1220.8789 - val_loss: 7.2569 - val_mse: 486.2583 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 13.1845 - mse: 1131.7699 - val_loss: 7.8110 - val_mse: 503.4924 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 12.4422 - mse: 946.4604 - val_loss: 7.6024 - val_mse: 507.6579 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 12.2579 - mse: 908.1302 - val_loss: 7.0544 - val_mse: 479.9651 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 11.7527 - mse: 815.2941 - val_loss: 7.1447 - val_mse: 483.2906 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 11.8284 - mse: 827.5510 - val_loss: 7.8628 - val_mse: 517.8635 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 31ms/step - loss: 11.6364 - mse: 787.3918 - val_loss: 7.3171 - val_mse: 487.0914 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 11.8369 - mse: 838.7047 - val_loss: 7.1746 - val_mse: 498.5520 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 11.3968 - mse: 746.5398 - val_loss: 9.8934 - val_mse: 660.3068 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 10.9879 - mse: 680.8381 - val_loss: 6.8092 - val_mse: 465.3875 - learning_rate: 5.0000e-04\n",
            "Epoch 14/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 10.7573 - mse: 651.1139 - val_loss: 6.4696 - val_mse: 437.3883 - learning_rate: 5.0000e-04\n",
            "Epoch 15/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 10.7004 - mse: 646.1968 - val_loss: 6.5263 - val_mse: 452.3572 - learning_rate: 5.0000e-04\n",
            "Epoch 16/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 10.5302 - mse: 611.5910 - val_loss: 6.3136 - val_mse: 433.8142 - learning_rate: 5.0000e-04\n",
            "Epoch 17/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 10.5158 - mse: 619.0201 - val_loss: 6.5385 - val_mse: 460.6841 - learning_rate: 5.0000e-04\n",
            "Epoch 18/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 30ms/step - loss: 10.5067 - mse: 608.5028 - val_loss: 6.5474 - val_mse: 449.1497 - learning_rate: 5.0000e-04\n",
            "Epoch 19/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 10.4382 - mse: 609.8226 - val_loss: 6.4825 - val_mse: 448.0396 - learning_rate: 5.0000e-04\n",
            "Epoch 20/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 10.2972 - mse: 576.9141 - val_loss: 6.5511 - val_mse: 446.3157 - learning_rate: 5.0000e-04\n",
            "Epoch 21/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 10.3814 - mse: 591.1603 - val_loss: 6.9155 - val_mse: 458.3803 - learning_rate: 5.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 10.1038 - mse: 541.8186 - val_loss: 6.3875 - val_mse: 438.7572 - learning_rate: 2.5000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 10.0169 - mse: 517.0193 - val_loss: 6.4879 - val_mse: 449.8179 - learning_rate: 2.5000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 10.0076 - mse: 512.1678 - val_loss: 6.4580 - val_mse: 448.8407 - learning_rate: 2.5000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 9.9541 - mse: 508.5878 - val_loss: 6.6016 - val_mse: 453.8718 - learning_rate: 2.5000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 9.9585 - mse: 504.8181 - val_loss: 6.1916 - val_mse: 432.0772 - learning_rate: 2.5000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 31ms/step - loss: 9.9692 - mse: 512.6569 - val_loss: 6.2916 - val_mse: 440.7939 - learning_rate: 2.5000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 9.9140 - mse: 497.3492 - val_loss: 6.2846 - val_mse: 439.5554 - learning_rate: 2.5000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 9.8646 - mse: 491.8492 - val_loss: 6.6933 - val_mse: 443.4994 - learning_rate: 2.5000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 9.9000 - mse: 496.7492 - val_loss: 6.3685 - val_mse: 438.1895 - learning_rate: 2.5000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 9.8998 - mse: 495.2874 - val_loss: 6.3592 - val_mse: 445.1817 - learning_rate: 2.5000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 9.7764 - mse: 472.6602 - val_loss: 6.3075 - val_mse: 440.2681 - learning_rate: 1.2500e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 9.7728 - mse: 473.5806 - val_loss: 6.1845 - val_mse: 430.7883 - learning_rate: 1.2500e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 9.7298 - mse: 469.4481 - val_loss: 6.2810 - val_mse: 433.3484 - learning_rate: 1.2500e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 9.7068 - mse: 460.9872 - val_loss: 6.2115 - val_mse: 430.6657 - learning_rate: 1.2500e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 9.6763 - mse: 452.7551 - val_loss: 6.2379 - val_mse: 432.7824 - learning_rate: 1.2500e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 9.6484 - mse: 445.5131 - val_loss: 6.2654 - val_mse: 439.4752 - learning_rate: 1.2500e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 31ms/step - loss: 9.6398 - mse: 436.9510 - val_loss: 6.5848 - val_mse: 449.1136 - learning_rate: 1.2500e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 9.5912 - mse: 428.3868 - val_loss: 6.2188 - val_mse: 436.1334 - learning_rate: 6.2500e-05\n",
            "Epoch 40/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 9.5813 - mse: 425.9209 - val_loss: 6.3159 - val_mse: 442.0500 - learning_rate: 6.2500e-05\n",
            "  Chosen mix: MSE=0.20, MAX=0.80 | threshold=124.393982 | val Sens=0.676, Spec=0.972\n",
            "  TAIL-ONLY → Sens=0.669, Spec=0.971, FPR=0.029, POD=0.955, Tim=0.201\n",
            "\n",
            "=== Signal 6 (LSTM AE; tail-only metrics) ===\n",
            "  Saved indices: val=200 → lstm_val_indices_sig6.csv,  test=200 → lstm_test_indices_sig6.csv\n",
            "  Train windows: 1302600 from first 6 years\n",
            "  Val windows (tail): 66000  positives: 1690 (2.6%)\n",
            "  Test windows (tail): 66000\n",
            "Epoch 1/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 30ms/step - loss: 1.7825 - mse: 16.2164 - val_loss: 1.2178 - val_mse: 8.0738 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 1.2116 - mse: 7.8908 - val_loss: 1.1375 - val_mse: 7.1735 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 1.1469 - mse: 7.1679 - val_loss: 1.0646 - val_mse: 6.5941 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 1.1160 - mse: 6.9299 - val_loss: 1.0518 - val_mse: 6.5570 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 1.0689 - mse: 6.3651 - val_loss: 1.0541 - val_mse: 6.4774 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 1.0726 - mse: 6.3649 - val_loss: 0.9878 - val_mse: 6.0153 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 1.0324 - mse: 5.9927 - val_loss: 0.9815 - val_mse: 6.0463 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 1.0393 - mse: 6.0416 - val_loss: 0.9754 - val_mse: 5.9524 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 1.0152 - mse: 5.8098 - val_loss: 0.9430 - val_mse: 5.6379 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 1.0030 - mse: 5.7089 - val_loss: 1.0995 - val_mse: 6.9744 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 1.0004 - mse: 5.6575 - val_loss: 0.9236 - val_mse: 5.5422 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 1.0055 - mse: 5.7715 - val_loss: 0.9493 - val_mse: 5.5926 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 0.9953 - mse: 5.6377 - val_loss: 0.9333 - val_mse: 5.5410 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 0.9977 - mse: 5.6289 - val_loss: 0.9793 - val_mse: 6.0118 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 0.9891 - mse: 5.5271 - val_loss: 0.9083 - val_mse: 5.3195 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 31ms/step - loss: 0.9705 - mse: 5.3538 - val_loss: 0.9128 - val_mse: 5.4603 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 0.9714 - mse: 5.3657 - val_loss: 0.9513 - val_mse: 5.6588 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 0.9613 - mse: 5.2694 - val_loss: 1.0291 - val_mse: 6.2700 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 30ms/step - loss: 1.0116 - mse: 5.7218 - val_loss: 0.8819 - val_mse: 5.1358 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 0.9827 - mse: 5.4685 - val_loss: 0.9516 - val_mse: 5.6431 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 0.9584 - mse: 5.2188 - val_loss: 0.9221 - val_mse: 5.4177 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 30ms/step - loss: 0.9700 - mse: 5.3278 - val_loss: 1.0174 - val_mse: 6.1194 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 0.9525 - mse: 5.1682 - val_loss: 1.0120 - val_mse: 6.4704 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 0.9581 - mse: 5.1996 - val_loss: 1.1183 - val_mse: 7.2674 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 30ms/step - loss: 0.9697 - mse: 5.3431 - val_loss: 0.8648 - val_mse: 4.9952 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 30ms/step - loss: 0.9233 - mse: 4.8701 - val_loss: 0.8486 - val_mse: 4.8742 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 0.9116 - mse: 4.7730 - val_loss: 0.8463 - val_mse: 4.8228 - learning_rate: 5.0000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 0.9047 - mse: 4.7129 - val_loss: 0.8477 - val_mse: 4.8017 - learning_rate: 5.0000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 0.9053 - mse: 4.7153 - val_loss: 0.8476 - val_mse: 4.8763 - learning_rate: 5.0000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 0.9024 - mse: 4.6837 - val_loss: 0.8390 - val_mse: 4.8565 - learning_rate: 5.0000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 31ms/step - loss: 0.8989 - mse: 4.6653 - val_loss: 0.8414 - val_mse: 4.7877 - learning_rate: 5.0000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 30ms/step - loss: 0.9036 - mse: 4.7146 - val_loss: 0.8511 - val_mse: 4.8313 - learning_rate: 5.0000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 30ms/step - loss: 0.9023 - mse: 4.7064 - val_loss: 0.8361 - val_mse: 4.7775 - learning_rate: 5.0000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 30ms/step - loss: 0.8947 - mse: 4.6574 - val_loss: 0.8299 - val_mse: 4.7610 - learning_rate: 5.0000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 30ms/step - loss: 0.8934 - mse: 4.6302 - val_loss: 0.8219 - val_mse: 4.6048 - learning_rate: 5.0000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 30ms/step - loss: 0.8887 - mse: 4.5945 - val_loss: 0.8513 - val_mse: 4.7637 - learning_rate: 5.0000e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 30ms/step - loss: 0.8878 - mse: 4.5638 - val_loss: 0.8272 - val_mse: 4.7136 - learning_rate: 5.0000e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 30ms/step - loss: 0.8884 - mse: 4.5835 - val_loss: 0.8625 - val_mse: 5.0209 - learning_rate: 5.0000e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 30ms/step - loss: 0.8961 - mse: 4.6439 - val_loss: 0.8351 - val_mse: 4.7515 - learning_rate: 5.0000e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 30ms/step - loss: 0.8877 - mse: 4.5514 - val_loss: 0.8251 - val_mse: 4.6802 - learning_rate: 5.0000e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 30ms/step - loss: 0.8726 - mse: 4.4290 - val_loss: 0.8109 - val_mse: 4.5708 - learning_rate: 2.5000e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 30ms/step - loss: 0.8673 - mse: 4.3857 - val_loss: 0.8102 - val_mse: 4.5663 - learning_rate: 2.5000e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 30ms/step - loss: 0.8687 - mse: 4.4065 - val_loss: 0.8071 - val_mse: 4.6279 - learning_rate: 2.5000e-04\n",
            "Epoch 44/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 30ms/step - loss: 0.8651 - mse: 4.3928 - val_loss: 0.8075 - val_mse: 4.6774 - learning_rate: 2.5000e-04\n",
            "Epoch 45/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 30ms/step - loss: 0.8590 - mse: 4.3466 - val_loss: 0.8046 - val_mse: 4.5882 - learning_rate: 2.5000e-04\n",
            "Epoch 46/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 30ms/step - loss: 0.8614 - mse: 4.3651 - val_loss: 0.7979 - val_mse: 4.5284 - learning_rate: 2.5000e-04\n",
            "Epoch 47/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 30ms/step - loss: 0.8609 - mse: 4.3834 - val_loss: 0.7970 - val_mse: 4.5148 - learning_rate: 2.5000e-04\n",
            "Epoch 48/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 29ms/step - loss: 0.8592 - mse: 4.3357 - val_loss: 0.8018 - val_mse: 4.5748 - learning_rate: 2.5000e-04\n",
            "Epoch 49/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 29ms/step - loss: 0.8569 - mse: 4.3298 - val_loss: 0.8060 - val_mse: 4.5599 - learning_rate: 2.5000e-04\n",
            "Epoch 50/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 29ms/step - loss: 0.8560 - mse: 4.3141 - val_loss: 0.7969 - val_mse: 4.4597 - learning_rate: 2.5000e-04\n",
            "Epoch 51/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 29ms/step - loss: 0.8539 - mse: 4.2946 - val_loss: 0.8066 - val_mse: 4.5946 - learning_rate: 2.5000e-04\n",
            "Epoch 52/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 29ms/step - loss: 0.8524 - mse: 4.2872 - val_loss: 0.8004 - val_mse: 4.5183 - learning_rate: 2.5000e-04\n",
            "Epoch 53/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 29ms/step - loss: 0.8558 - mse: 4.3052 - val_loss: 0.8169 - val_mse: 4.7298 - learning_rate: 2.5000e-04\n",
            "Epoch 54/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 29ms/step - loss: 0.8565 - mse: 4.3476 - val_loss: 0.8114 - val_mse: 4.6043 - learning_rate: 2.5000e-04\n",
            "Epoch 55/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 29ms/step - loss: 0.8615 - mse: 4.3720 - val_loss: 0.7994 - val_mse: 4.5027 - learning_rate: 2.5000e-04\n",
            "Epoch 56/60\n",
            "\u001b[1m10177/10177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 30ms/step - loss: 0.8451 - mse: 4.2272 - val_loss: 0.7909 - val_mse: 4.4607 - learning_rate: 1.2500e-04\n",
            "Epoch 57/60\n",
            "\u001b[1m 3457/10177\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:13\u001b[0m 29ms/step - loss: 0.8400 - mse: 4.1792"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "LSTM Autoencoder — Tail-only metrics, using IsolationForest per-sim splits\n",
        "- Uses IsolationForest_per_sim_val.csv and IsolationForest_per_sim_test.csv to define splits.\n",
        "  If a 'split' column exists with values {train,val,test}, those are used directly\n",
        "  (supports 60/20/20 from IF). If absent, rows from the *_val.csv are 'val' and\n",
        "  *_test.csv are 'test', and TRAIN = remaining sims.\n",
        "- Unsupervised training on first 6 years from TRAIN sims\n",
        "- Validation tuner (mix + threshold), optional guard-rail threshold (train 95th pct for chosen mix)\n",
        "- Evaluates tail-only comparator metrics (last 49 weeks) with FULL per-day coverage (no front padding)\n",
        "\n",
        "Files written:\n",
        "  - LSTM_AE_tail_only_metrics.csv\n",
        "  - (optional confirmation) lstm_val_indices_sig{S}.csv, lstm_test_indices_sig{S}.csv\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "DATA_DIR      = \"\"   # <-- set folder with simulated_totals_sig{S}.csv & simulated_outbreaks_sig{S}.csv\n",
        "VAL_SPLIT_CSV = \"IsolationForest_big_medium_per_sim_val.csv\"\n",
        "TEST_SPLIT_CSV= \"IsolationForest_big_medium_per_sim_test.csv\"\n",
        "\n",
        "SIGNALS       = list(range(5, 9))\n",
        "DAYS_PER_YEAR = 364\n",
        "SEQ, STRIDE   = 14, 1\n",
        "TRAIN_YEARS   = 6\n",
        "TRAIN_DAYS    = TRAIN_YEARS * DAYS_PER_YEAR\n",
        "TAIL_DAYS     = 49 * 7   # 343 (last 49 weeks)\n",
        "\n",
        "# Training\n",
        "EPOCHS        = 60\n",
        "BATCH_SIZE    = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "PATIENCE      = 7\n",
        "MIN_EPOCHS    = 20\n",
        "RNG_STATE     = 42\n",
        "\n",
        "# Validation tuning\n",
        "SPEC_TARGET       = 0.97\n",
        "TUNE_WEIGHT_SENS  = 2.0\n",
        "TUNE_WEIGHT_SPEC  = 3.0\n",
        "\n",
        "MIX_GRID = (\n",
        "    (1.00, 0.00), (0.95, 0.05), (0.90, 0.10),\n",
        "    (0.80, 0.20), (0.70, 0.30), (0.60, 0.40),\n",
        "    (0.40, 0.60), (0.30, 0.70), (0.20, 0.80),\n",
        "    (0.10, 0.90), (0.05, 0.95), (0.00, 1.00),\n",
        ")\n",
        "ANOMALY_MIX_MSE   = 0.8\n",
        "ANOMALY_MIX_MAX   = 0.2\n",
        "\n",
        "np.random.seed(RNG_STATE)\n",
        "tf.random.set_seed(RNG_STATE)\n",
        "\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices(\"GPU\"))\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    except Exception as e:\n",
        "        print(\"Could not set memory growth:\", e)\n",
        "\n",
        "# ========== SPLIT HELPERS ==========\n",
        "SIM_RE = re.compile(r\"^sig(?P<sig>\\d+)_sim(?P<idx>\\d+)$\")\n",
        "\n",
        "def parse_sim_name(sim_name):\n",
        "    m = SIM_RE.match(sim_name)\n",
        "    if not m:\n",
        "        return None, None\n",
        "    return int(m.group(\"sig\")), int(m.group(\"idx\"))\n",
        "\n",
        "def load_if_splits(val_csv, test_csv):\n",
        "    \"\"\"\n",
        "    Return dict: signal -> {'train': set(sim_names), 'val': set(sim_names), 'test': set(sim_names)}.\n",
        "\n",
        "    If CSVs include a 'split' column with values in {train,val,test}, those are used directly\n",
        "    (supports a single large-file style or explicit 60/20/20 lists stored across the two CSVs).\n",
        "    If 'split' is absent, rows from `val_csv` are treated as 'val', rows from `test_csv` as 'test',\n",
        "    and TRAIN will be derived as the leftover sims present in the data.\n",
        "    \"\"\"\n",
        "    def _load(path, default_split):\n",
        "        df = pd.read_csv(path)\n",
        "        if \"signal\" not in df.columns or \"sim\" not in df.columns:\n",
        "            raise ValueError(f\"{path} must contain columns ['signal','sim', ...]\")\n",
        "        split_col = df[\"split\"].astype(str).str.lower() if \"split\" in df.columns else pd.Series([default_split]*len(df))\n",
        "        return pd.DataFrame({\n",
        "            \"signal\": df[\"signal\"].astype(int),\n",
        "            \"sim\": df[\"sim\"].astype(str),\n",
        "            \"split\": split_col\n",
        "        })\n",
        "\n",
        "    dfv = _load(val_csv,  default_split=\"val\")\n",
        "    dft = _load(test_csv, default_split=\"test\")\n",
        "    df  = pd.concat([dfv, dft], ignore_index=True)\n",
        "\n",
        "    by_signal = {}\n",
        "    for _, row in df.iterrows():\n",
        "        sig = int(row[\"signal\"]); sim = str(row[\"sim\"]); split = str(row[\"split\"]).lower()\n",
        "        d = by_signal.setdefault(sig, {\"train\": set(), \"val\": set(), \"test\": set()})\n",
        "        if split in {\"train\", \"val\", \"test\"}:\n",
        "            d[split].add(sim)\n",
        "    return by_signal\n",
        "\n",
        "# ========== DATA HELPERS ==========\n",
        "def load_signal(sig):\n",
        "    x_path = os.path.join(DATA_DIR, f\"simulated_totals_sig{sig}.csv\")\n",
        "    y_path = os.path.join(DATA_DIR, f\"simulated_outbreaks_sig{sig}.csv\")\n",
        "    if not (os.path.exists(x_path) and os.path.exists(y_path)):\n",
        "        raise FileNotFoundError(f\"Missing CSVs for signal {sig}: {x_path}, {y_path}\")\n",
        "    X = pd.read_csv(x_path)\n",
        "    Y = pd.read_csv(y_path)\n",
        "    for c in list(X.columns):\n",
        "        if c.lower() in (\"date\", \"ds\", \"timestamp\"):\n",
        "            X = X.drop(columns=[c])\n",
        "            if c in Y.columns:\n",
        "                Y = Y.drop(columns=[c])\n",
        "            break\n",
        "    X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    Y = (Y.apply(pd.to_numeric, errors=\"coerce\") > 0).astype(int)\n",
        "    return X, Y\n",
        "\n",
        "def make_seq_labels(series, labels, seq_len=SEQ, stride=STRIDE):\n",
        "    X, Y = [], []\n",
        "    for i in range(0, len(series) - seq_len + 1, stride):\n",
        "        X.append(series[i:i+seq_len])\n",
        "        Y.append(labels[i+seq_len-1])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "# ========== MODEL ==========\n",
        "def build_lstm_autoencoder(seq_len=SEQ, learning_rate=LEARNING_RATE):\n",
        "    inputs = tf.keras.Input(shape=(seq_len, 1))\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x); x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x); x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=False))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x); x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    bottleneck = tf.keras.layers.Dense(4, activation=\"linear\")(x)\n",
        "    x = tf.keras.layers.RepeatVector(seq_len)(bottleneck)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x); x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x); x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x); x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation=\"linear\"))(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss=\"huber\", metrics=[\"mse\"])\n",
        "    return model\n",
        "\n",
        "# ========== TUNING HELPERS ==========\n",
        "def sens_spec_for_tuning(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
        "    s  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
        "    sp = tn/(tn+fp) if (tn+fp)>0 else 0.0\n",
        "    return s, sp\n",
        "\n",
        "def tune_aggressive_threshold(y_val, anomaly_scores, spec_target=SPEC_TARGET, w_sens=TUNE_WEIGHT_SENS, w_spec=TUNE_WEIGHT_SPEC):\n",
        "    best_t, best_score = None, -1.0\n",
        "    for p in [85,87,89,90,91,92,93,94,95,96,97,98,98.5,99,99.2,99.5]:\n",
        "        t = np.percentile(anomaly_scores, p)\n",
        "        yhat = (anomaly_scores >= t).astype(int)\n",
        "        s, sp = sens_spec_for_tuning(y_val, yhat)\n",
        "        if sp >= spec_target:\n",
        "            score = w_sens*s + w_spec*sp\n",
        "            if score > best_score:\n",
        "                best_score, best_t = score, float(t)\n",
        "    if best_t is None:\n",
        "        for p in [80,82,84,85,86,87,88,89,90,91,92,93,94]:\n",
        "            t = np.percentile(anomaly_scores, p)\n",
        "            yhat = (anomaly_scores >= t).astype(int)\n",
        "            s, sp = sens_spec_for_tuning(y_val, yhat)\n",
        "            if sp >= spec_target:\n",
        "                score = w_sens*s + w_spec*sp\n",
        "                if score > best_score:\n",
        "                    best_score, best_t = score, float(t)\n",
        "    if best_t is None:\n",
        "        best_t = float(np.percentile(anomaly_scores, 80))\n",
        "        print(\"WARNING: No threshold achieved target specificity; using conservative 80th pct.\")\n",
        "    return best_t\n",
        "\n",
        "def tune_mix_and_threshold(Xval, Yval, recon_val,\n",
        "                           mixes=MIX_GRID, spec_target=SPEC_TARGET,\n",
        "                           w_sens=TUNE_WEIGHT_SENS, w_spec=TUNE_WEIGHT_SPEC):\n",
        "    val_mse = np.mean((Xval - recon_val)**2, axis=(1,2))\n",
        "    val_max = np.max(np.abs(Xval - recon_val), axis=(1,2))\n",
        "    best = dict(score=-1.0, mix=None, threshold=None, sens=None, spec=None)\n",
        "    for m_mse, m_max in mixes:\n",
        "        scores = m_mse*val_mse + m_max*val_max\n",
        "        thr = tune_aggressive_threshold(Yval, scores, spec_target, w_sens, w_spec)\n",
        "        yhat = (scores >= thr).astype(int)\n",
        "        s, sp = sens_spec_for_tuning(Yval, yhat)\n",
        "        if sp >= spec_target:\n",
        "            score = w_sens*s + w_spec*sp\n",
        "            if score > best[\"score\"]:\n",
        "                best.update(score=score, mix=(m_mse, m_max), threshold=thr, sens=s, spec=sp)\n",
        "    if best[\"mix\"] is None:\n",
        "        best_spec = -1.0\n",
        "        for m_mse, m_max in mixes:\n",
        "            scores = m_mse*val_mse + m_max*val_max\n",
        "            thr = float(np.percentile(scores, 80))\n",
        "            yhat = (scores >= thr).astype(int)\n",
        "            s, sp = sens_spec_for_tuning(Yval, yhat)\n",
        "            if sp > best_spec:\n",
        "                best_spec = sp\n",
        "                best.update(score=w_sens*s + w_spec*sp, mix=(m_mse, m_max),\n",
        "                            threshold=thr, sens=s, spec=sp)\n",
        "        print(\"NOTE: no mix hit the specificity target; chose the most specific fallback.\")\n",
        "    return best\n",
        "\n",
        "# ========== COMPARATOR METRICS (TAIL-ONLY) ==========\n",
        "def compute_fpr_tail(A_tail, O_tail):\n",
        "    FP = np.sum((A_tail == 1) & (O_tail == 0))\n",
        "    N0 = np.sum(O_tail == 0)\n",
        "    return (FP / N0) if N0 > 0 else np.nan\n",
        "\n",
        "def compute_specificity_tail(A_tail, O_tail):\n",
        "    TN = np.sum((A_tail == 0) & (O_tail == 0))\n",
        "    N0 = np.sum(O_tail == 0)\n",
        "    return (TN / N0) if N0 > 0 else np.nan\n",
        "\n",
        "def compute_sensitivity_tail(A_tail, O_tail):\n",
        "    TP = np.sum((A_tail == 1) & (O_tail > 0))\n",
        "    P  = np.sum(O_tail > 0)\n",
        "    return (TP / P) if P > 0 else np.nan\n",
        "\n",
        "def compute_pod_tail(A_tail, O_tail):\n",
        "    nsim = A_tail.shape[1]\n",
        "    hits = [np.any((A_tail[:, j] == 1) & (O_tail[:, j] > 0)) for j in range(nsim)]\n",
        "    return float(np.mean(hits)) if nsim > 0 else np.nan\n",
        "\n",
        "def compute_timeliness_tail(A_tail, O_tail):\n",
        "    nsim = A_tail.shape[1]\n",
        "    score = 0.0\n",
        "    miss  = 0\n",
        "    for j in range(nsim):\n",
        "        o = O_tail[:, j]\n",
        "        if np.sum(o > 0) == 0:\n",
        "            miss += 1\n",
        "            continue\n",
        "        r_idx = np.where(o > 0)[0]\n",
        "        r1, r2 = int(r_idx[0]), int(r_idx[-1])\n",
        "        a = A_tail[:, j]\n",
        "        hit_idx = np.where((a == 1) & (o > 0))[0]\n",
        "        if len(hit_idx) == 0:\n",
        "            miss += 1\n",
        "        else:\n",
        "            score += (int(hit_idx[0]) - r1) / (r2 - r1 + 1)\n",
        "    return (score + miss) / nsim if nsim > 0 else np.nan\n",
        "\n",
        "# ========== ES with min-epoch floor ==========\n",
        "class EarlyStoppingWithMin(tf.keras.callbacks.EarlyStopping):\n",
        "    def __init__(self, min_epochs=MIN_EPOCHS, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.min_epochs = min_epochs\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) < self.min_epochs:\n",
        "            return\n",
        "        return super().on_epoch_end(epoch, logs)\n",
        "\n",
        "# ========== MAIN ==========\n",
        "rng = np.random.RandomState(RNG_STATE)\n",
        "summary_rows = []\n",
        "\n",
        "# Load IF-driven splits once (supports explicit 60/20/20 if present)\n",
        "print(f\"Loading predefined IF splits:\\n  VAL:  {VAL_SPLIT_CSV}\\n  TEST: {TEST_SPLIT_CSV}\")\n",
        "by_signal = load_if_splits(VAL_SPLIT_CSV, TEST_SPLIT_CSV)\n",
        "\n",
        "for S in SIGNALS:\n",
        "    print(f\"\\n=== Signal {S} (LSTM AE; IF-driven splits; tail-only metrics) ===\")\n",
        "    Xdf, Ydf = load_signal(S)\n",
        "\n",
        "    # Build sim list from all columns\n",
        "    sims = []\n",
        "    for sim_idx, col in enumerate(Xdf.columns):\n",
        "        x = Xdf[col].to_numpy(dtype=float)\n",
        "        y = Ydf[col].to_numpy(dtype=int)\n",
        "        sim_name = f\"sig{S}_sim{sim_idx}\"\n",
        "        sims.append(dict(x=x, y=y, name=sim_name, col_idx=sim_idx))\n",
        "\n",
        "    # Pick train/val/test sims from the IF files (prefer explicit 'train' if provided)\n",
        "    sig_splits = by_signal.get(S, {\"train\": set(), \"val\": set(), \"test\": set()})\n",
        "    want_train = set(sig_splits.get(\"train\", set()))\n",
        "    want_val   = set(sig_splits.get(\"val\", set()))\n",
        "    want_test  = set(sig_splits.get(\"test\", set()))\n",
        "\n",
        "    train_sims, val_sims, test_sims = [], [], []\n",
        "    missing_train, missing_val, missing_test = set(), set(), set()\n",
        "    sim_by_name = {d[\"name\"]: d for d in sims}\n",
        "\n",
        "    use_explicit_train = len(want_train) > 0\n",
        "\n",
        "    if use_explicit_train:\n",
        "        for name in sorted(want_train):\n",
        "            d = sim_by_name.get(name)\n",
        "            if d is None: missing_train.add(name); continue\n",
        "            if len(d[\"x\"]) < (TRAIN_DAYS + TAIL_DAYS): continue\n",
        "            train_sims.append(d)\n",
        "\n",
        "    for name in sorted(want_val):\n",
        "        d = sim_by_name.get(name)\n",
        "        if d is None: missing_val.add(name); continue\n",
        "        if len(d[\"x\"]) < (TRAIN_DAYS + TAIL_DAYS): print(f\"  ⚠ Skipping VAL {name}: too short\"); continue\n",
        "        val_sims.append(d)\n",
        "\n",
        "    for name in sorted(want_test):\n",
        "        d = sim_by_name.get(name)\n",
        "        if d is None: missing_test.add(name); continue\n",
        "        if len(d[\"x\"]) < (TRAIN_DAYS + TAIL_DAYS): print(f\"  ⚠ Skipping TEST {name}: too short\"); continue\n",
        "        test_sims.append(d)\n",
        "\n",
        "    if not use_explicit_train:\n",
        "        picked = {d[\"name\"] for d in val_sims} | {d[\"name\"] for d in test_sims}\n",
        "        for d in sims:\n",
        "            if d[\"name\"] in picked: continue\n",
        "            if len(d[\"x\"]) >= (TRAIN_DAYS + TAIL_DAYS): train_sims.append(d)\n",
        "\n",
        "    print(f\"  Using splits (from IF files): {len(train_sims)} train, {len(val_sims)} val, {len(test_sims)} test\")\n",
        "    if missing_train: print(f\"  ⚠ Missing in data (TRAIN): {sorted(missing_train)[:5]}{' ...' if len(missing_train)>5 else ''}\")\n",
        "    if missing_val:  print(f\"  ⚠ Missing in data (VAL):   {sorted(missing_val)[:5]}{' ...' if len(missing_val)>5 else ''}\")\n",
        "    if missing_test: print(f\"  ⚠ Missing in data (TEST):  {sorted(missing_test)[:5]}{' ...' if len(missing_test)>5 else ''}\")\n",
        "\n",
        "    # Optional confirmation dump\n",
        "    pd.Series([d[\"col_idx\"] for d in val_sims]).to_csv(f\"lstm_val_indices_sig{S}.csv\", index=False, header=False)\n",
        "    pd.Series([d[\"col_idx\"] for d in test_sims]).to_csv(f\"lstm_test_indices_sig{S}.csv\", index=False, header=False)\n",
        "\n",
        "    if not train_sims:\n",
        "        print(\"  No training sims with enough length; skipping.\"); continue\n",
        "    if not test_sims:\n",
        "        print(\"  No test sims with enough length; skipping.\"); continue\n",
        "\n",
        "    # ---- Unsupervised TRAIN windows from first 6y\n",
        "    Xtr_list = []\n",
        "    for d in train_sims:\n",
        "        x_train = d[\"x\"][:TRAIN_DAYS]\n",
        "        X_seq, _ = make_seq_labels(x_train, np.zeros_like(x_train), SEQ, STRIDE)\n",
        "        if len(X_seq): Xtr_list.append(X_seq)\n",
        "    if not Xtr_list:\n",
        "        print(\"  No training windows; skipping.\"); continue\n",
        "    Xtr = np.concatenate(Xtr_list).reshape(-1, SEQ, 1).astype(np.float32)\n",
        "    print(f\"  Train windows: {len(Xtr)} from first {TRAIN_YEARS} years\")\n",
        "\n",
        "    # ---- VALIDATION: build with (SEQ-1) days of CONTEXT so we get 1 score per tail day (no padding)\n",
        "    Xv_list, Yv_list = [], []\n",
        "    for d in val_sims:\n",
        "        x = d[\"x\"]; y = d[\"y\"]\n",
        "        tail_start = len(x) - TAIL_DAYS\n",
        "        ctx_start  = tail_start - (SEQ - 1)\n",
        "        if ctx_start < 0: continue\n",
        "        x_ctx_tail = x[ctx_start : tail_start + TAIL_DAYS]\n",
        "        y_ctx_tail = y[ctx_start : tail_start + TAIL_DAYS]\n",
        "        Xv, Yv = make_seq_labels(x_ctx_tail, y_ctx_tail, SEQ, STRIDE)  # len == TAIL_DAYS\n",
        "        if len(Xv) == TAIL_DAYS:\n",
        "            Xv_list.append(Xv); Yv_list.append(Yv)\n",
        "    Xval = np.concatenate(Xv_list).reshape(-1, SEQ, 1).astype(np.float32) if Xv_list else np.empty((0,SEQ,1), np.float32)\n",
        "    Yval = np.concatenate(Yv_list).astype(int) if Yv_list else np.empty((0,), np.int32)\n",
        "    if len(Yval):\n",
        "        pos = int(Yval.sum()); pct = 100*Yval.mean()\n",
        "        print(f\"  Val windows (tail, full coverage): {len(Xval)}  positives: {pos} ({pct:.1f}%)\")\n",
        "    else:\n",
        "        print(\"  No val windows (tuner will fall back to train guard-rail).\")\n",
        "\n",
        "    # ---- TEST: same context trick → exactly TAIL_DAYS scores per sim, no front padding\n",
        "    per_sim_labels = []   # each length = TAIL_DAYS\n",
        "    A_tail_cols    = []   # alarms per sim (343)\n",
        "    O_tail_cols    = []   # labels per sim (343)\n",
        "    Xte_chunks     = []   # each chunk length = TAIL_DAYS (of SEQ-length windows)\n",
        "\n",
        "    for d in test_sims:\n",
        "        x = d[\"x\"]; y = d[\"y\"]\n",
        "        tail_start = len(x) - TAIL_DAYS\n",
        "        ctx_start  = tail_start - (SEQ - 1)\n",
        "        if ctx_start < 0: continue\n",
        "        x_ctx_tail = x[ctx_start : tail_start + TAIL_DAYS]\n",
        "        y_tail     = y[tail_start : tail_start + TAIL_DAYS].astype(int)  # 343 labels aligned to tail days\n",
        "        Xte, _     = make_seq_labels(x_ctx_tail, y[ctx_start : tail_start + TAIL_DAYS], SEQ, STRIDE)  # len == 343\n",
        "        if len(Xte) == TAIL_DAYS:\n",
        "            Xte_chunks.append(Xte)\n",
        "            per_sim_labels.append(y_tail)\n",
        "\n",
        "    if not Xte_chunks:\n",
        "        print(\"  No test windows; skipping.\"); continue\n",
        "\n",
        "    Xte = np.concatenate(Xte_chunks).reshape(-1, SEQ, 1).astype(np.float32)\n",
        "    print(f\"  Test windows (tail, full coverage): {len(Xte)}\")\n",
        "\n",
        "    # ---- Build & train\n",
        "    model = build_lstm_autoencoder(SEQ, LEARNING_RATE)\n",
        "    if len(Xval):\n",
        "        cbs = [\n",
        "            EarlyStoppingWithMin(min_epochs=MIN_EPOCHS, patience=PATIENCE, restore_best_weights=True, monitor=\"val_loss\"),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6),\n",
        "            tf.keras.callbacks.TerminateOnNaN(),\n",
        "        ]\n",
        "        model.fit(Xtr, Xtr, validation_data=(Xval, Xval), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=cbs, verbose=1)\n",
        "    else:\n",
        "        cbs = [\n",
        "            EarlyStoppingWithMin(min_epochs=MIN_EPOCHS, patience=PATIENCE, restore_best_weights=True, monitor=\"loss\"),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=5, min_lr=1e-6),\n",
        "            tf.keras.callbacks.TerminateOnNaN(),\n",
        "        ]\n",
        "        model.fit(Xtr, Xtr, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=cbs, verbose=1)\n",
        "\n",
        "    # ---- Validation: tune mix + threshold (or fallback if no val)\n",
        "    if len(Xval):\n",
        "        val_rec = model.predict(Xval, batch_size=BATCH_SIZE, verbose=0)\n",
        "        best = tune_mix_and_threshold(\n",
        "            Xval, Yval, val_rec,\n",
        "            mixes=MIX_GRID, spec_target=SPEC_TARGET,\n",
        "            w_sens=TUNE_WEIGHT_SENS, w_spec=TUNE_WEIGHT_SPEC\n",
        "        )\n",
        "        AM_MSE, AM_MAX = best[\"mix\"]\n",
        "        best_threshold = best[\"threshold\"]\n",
        "\n",
        "        # Guard-rail on TRAIN with chosen mix\n",
        "        tr_rec_for_guard = model.predict(Xtr, batch_size=BATCH_SIZE, verbose=0)\n",
        "        tr_mse_g = np.mean((Xtr - tr_rec_for_guard)**2, axis=(1,2))\n",
        "        tr_max_g = np.max(np.abs(Xtr - tr_rec_for_guard), axis=(1,2))\n",
        "        tr_scores_g = AM_MSE*tr_mse_g + AM_MAX*tr_max_g\n",
        "        guardrail_thr = float(np.percentile(tr_scores_g, 95))\n",
        "        if best_threshold < guardrail_thr:\n",
        "            print(f\"  Guard-rail raised threshold: {best_threshold:.6f} -> {guardrail_thr:.6f}\")\n",
        "            best_threshold = guardrail_thr\n",
        "\n",
        "        print(f\"  Chosen mix: MSE={AM_MSE:.2f}, MAX={AM_MAX:.2f} | \"\n",
        "              f\"threshold={best_threshold:.6f} | val Sens={best['sens']:.3f}, Spec={best['spec']:.3f}\")\n",
        "    else:\n",
        "        tr_rec = model.predict(Xtr, batch_size=BATCH_SIZE, verbose=0)\n",
        "        tr_mse = np.mean((Xtr - tr_rec)**2, axis=(1,2))\n",
        "        best_threshold = float(np.percentile(tr_mse, 95))\n",
        "        AM_MSE, AM_MAX = ANOMALY_MIX_MSE, ANOMALY_MIX_MAX\n",
        "        print(f\"  No validation; using default mix MSE={AM_MSE:.2f}, MAX={AM_MAX:.2f} and 95th pct train MSE threshold.\")\n",
        "\n",
        "    # ---- TEST inference (one score per tail day per sim; no padding)\n",
        "    te_rec  = model.predict(Xte, batch_size=BATCH_SIZE, verbose=0)\n",
        "    te_mse  = np.mean((Xte - te_rec)**2, axis=(1,2))\n",
        "    te_max  = np.max(np.abs(Xte - te_rec), axis=(1,2))\n",
        "    te_scores = AM_MSE*te_mse + AM_MAX*te_max  # length = TAIL_DAYS * n_test_sims\n",
        "\n",
        "    # Chunk back per sim (each chunk length = TAIL_DAYS)\n",
        "    ofs = 0\n",
        "    for y_tail in per_sim_labels:\n",
        "        yhat_seq = (te_scores[ofs:ofs+TAIL_DAYS] >= best_threshold).astype(int)\n",
        "        ofs += TAIL_DAYS\n",
        "        A_tail_cols.append(yhat_seq)           # length 343\n",
        "        O_tail_cols.append(y_tail.astype(int)) # length 343\n",
        "\n",
        "    if not A_tail_cols:\n",
        "        print(\"  No test sims; skipping metrics.\"); continue\n",
        "\n",
        "    A_tail = np.column_stack(A_tail_cols)  # [343, nsim_test]\n",
        "    O_tail = np.column_stack(O_tail_cols)  # [343, nsim_test]\n",
        "\n",
        "    # ---- Tail-only metrics\n",
        "    sens = compute_sensitivity_tail(A_tail, O_tail)\n",
        "    spec = compute_specificity_tail(A_tail, O_tail)\n",
        "    fpr  = compute_fpr_tail(A_tail, O_tail)\n",
        "    pod  = compute_pod_tail(A_tail, O_tail)\n",
        "    tim  = compute_timeliness_tail(A_tail, O_tail)\n",
        "\n",
        "    print(f\"  TAIL-ONLY → Sens={sens:.3f}, Spec={spec:.3f}, FPR={fpr:.3f}, POD={pod:.3f}, Tim={tim:.3f}\")\n",
        "\n",
        "    summary_rows.append(dict(\n",
        "        signal=S, sensitivity=sens, specificity=spec, fpr=fpr, pod=pod, timeliness=tim,\n",
        "        threshold=best_threshold, mix_mse=AM_MSE, mix_max=AM_MAX, nsim_test=A_tail.shape[1]\n",
        "    ))\n",
        "\n",
        "# ========== SAVE SUMMARY ==========\n",
        "if summary_rows:\n",
        "    df = pd.DataFrame(summary_rows).set_index(\"signal\").sort_index()\n",
        "    print(\"\\n=== LSTM Autoencoder — Tail-only Comparator Metrics (by signal) ===\")\n",
        "    print(df.round(6))\n",
        "    print(\"\\n=== Means ===\")\n",
        "    print(df[[\"sensitivity\",\"specificity\",\"fpr\",\"pod\",\"timeliness\"]].mean().round(6))\n",
        "    df.to_csv(\"LSTM_AE_tail_only_metrics.csv\")\n",
        "    print(\"\\nSaved: LSTM_AE_tail_only_metrics.csv\")\n",
        "else:\n",
        "    print(\"\\nNo results to summarize.\")\n"
      ]
    }
  ]
}