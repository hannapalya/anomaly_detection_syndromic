{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOYzseuLbtjjN/diIvvKLUB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannapalya/anomaly_detection_syndromic/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuYnY9zfNGAC",
        "outputId": "93d039be-636e-4ac4-8c87-918e5f2f24c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Loading predefined IF splits:\n",
            "  VAL:  IsolationForest_big_medium_per_sim_val.csv\n",
            "  TEST: IsolationForest_big_medium_per_sim_test.csv\n",
            "\n",
            "=== Signal 5 (LSTM AE; IF-driven splits; tail-only metrics) ===\n",
            "  Using splits (from IF files): 300 train, 100 val, 100 test\n",
            "  Train windows: 651300 from first 6 years\n",
            "  Val windows (tail, full coverage): 34300  positives: 1597 (4.7%)\n",
            "  Test windows (tail, full coverage): 34300\n",
            "Epoch 1/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 30ms/step - loss: 44.4218 - mse: 10864.1592 - val_loss: 16.9603 - val_mse: 1597.6184 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 30ms/step - loss: 18.2038 - mse: 2036.0233 - val_loss: 9.1515 - val_mse: 763.8326 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 16.4785 - mse: 1752.8055 - val_loss: 9.4125 - val_mse: 723.4474 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 16.2375 - mse: 1706.8126 - val_loss: 8.4430 - val_mse: 659.9064 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 15.2089 - mse: 1515.0818 - val_loss: 8.0315 - val_mse: 585.4103 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 29ms/step - loss: 14.9614 - mse: 1414.2290 - val_loss: 8.2374 - val_mse: 600.3148 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 30ms/step - loss: 13.9262 - mse: 1238.4463 - val_loss: 11.1774 - val_mse: 856.2539 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 14.1373 - mse: 1285.5776 - val_loss: 7.6392 - val_mse: 480.6131 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 30ms/step - loss: 13.8067 - mse: 1288.1580 - val_loss: 10.8239 - val_mse: 661.8138 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 29ms/step - loss: 13.2299 - mse: 1103.0828 - val_loss: 7.1126 - val_mse: 467.3379 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 29ms/step - loss: 12.8858 - mse: 1072.2727 - val_loss: 7.2229 - val_mse: 444.6860 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 13.0861 - mse: 1101.4150 - val_loss: 7.5559 - val_mse: 553.7479 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 30ms/step - loss: 12.5217 - mse: 984.9896 - val_loss: 7.0904 - val_mse: 469.9652 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 12.1868 - mse: 918.5580 - val_loss: 14.8473 - val_mse: 3031.5825 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 30ms/step - loss: 14.8823 - mse: 1515.1349 - val_loss: 7.4419 - val_mse: 502.8211 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 30ms/step - loss: 13.5880 - mse: 1203.1864 - val_loss: 7.4099 - val_mse: 462.7289 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 12.7956 - mse: 1026.1368 - val_loss: 7.3526 - val_mse: 478.3283 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 12.7828 - mse: 967.7817 - val_loss: 7.2090 - val_mse: 491.1287 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 11.7444 - mse: 818.3368 - val_loss: 8.5777 - val_mse: 536.2930 - learning_rate: 5.0000e-04\n",
            "Epoch 20/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 11.4419 - mse: 755.8824 - val_loss: 6.7610 - val_mse: 443.1702 - learning_rate: 5.0000e-04\n",
            "Epoch 21/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 11.3621 - mse: 751.0715 - val_loss: 6.6126 - val_mse: 446.9613 - learning_rate: 5.0000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 11.3106 - mse: 736.5535 - val_loss: 6.3372 - val_mse: 433.5577 - learning_rate: 5.0000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 11.3998 - mse: 756.3080 - val_loss: 7.5305 - val_mse: 495.5400 - learning_rate: 5.0000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 11.1575 - mse: 720.3413 - val_loss: 6.2979 - val_mse: 427.6702 - learning_rate: 5.0000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 30ms/step - loss: 11.0684 - mse: 702.6792 - val_loss: 6.2993 - val_mse: 425.9222 - learning_rate: 5.0000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 31ms/step - loss: 10.9555 - mse: 669.5094 - val_loss: 7.9757 - val_mse: 577.8040 - learning_rate: 5.0000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 11.0083 - mse: 686.6715 - val_loss: 6.2648 - val_mse: 426.8295 - learning_rate: 5.0000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 10.9318 - mse: 680.3350 - val_loss: 6.4445 - val_mse: 438.3612 - learning_rate: 5.0000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 10.9802 - mse: 678.6882 - val_loss: 6.3194 - val_mse: 434.3889 - learning_rate: 5.0000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 10.7752 - mse: 619.8242 - val_loss: 6.3935 - val_mse: 440.8917 - learning_rate: 5.0000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 10.6898 - mse: 595.6857 - val_loss: 6.3123 - val_mse: 429.6587 - learning_rate: 5.0000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 10.7014 - mse: 597.7628 - val_loss: 6.6549 - val_mse: 432.2860 - learning_rate: 5.0000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 10.4482 - mse: 553.2816 - val_loss: 6.0902 - val_mse: 427.9351 - learning_rate: 2.5000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 10.3479 - mse: 527.1319 - val_loss: 6.1507 - val_mse: 431.6035 - learning_rate: 2.5000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 31ms/step - loss: 10.2836 - mse: 515.3599 - val_loss: 7.3948 - val_mse: 468.5871 - learning_rate: 2.5000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 10.5131 - mse: 550.8318 - val_loss: 6.2016 - val_mse: 432.7119 - learning_rate: 2.5000e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 10.2995 - mse: 518.1380 - val_loss: 6.0774 - val_mse: 420.6015 - learning_rate: 2.5000e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 29ms/step - loss: 10.2245 - mse: 502.0119 - val_loss: 6.0310 - val_mse: 418.9730 - learning_rate: 2.5000e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 10.1663 - mse: 489.7846 - val_loss: 6.1736 - val_mse: 433.5703 - learning_rate: 2.5000e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 10.1596 - mse: 484.7281 - val_loss: 6.0860 - val_mse: 429.5016 - learning_rate: 2.5000e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 10.1464 - mse: 490.0800 - val_loss: 6.0662 - val_mse: 420.5827 - learning_rate: 2.5000e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 30ms/step - loss: 10.1553 - mse: 487.4259 - val_loss: 6.0310 - val_mse: 426.3733 - learning_rate: 2.5000e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 29ms/step - loss: 10.1303 - mse: 483.2922 - val_loss: 6.4620 - val_mse: 447.6843 - learning_rate: 2.5000e-04\n",
            "Epoch 44/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 10.0304 - mse: 468.9314 - val_loss: 6.0015 - val_mse: 426.0348 - learning_rate: 1.2500e-04\n",
            "Epoch 45/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 9.9970 - mse: 465.5092 - val_loss: 6.0789 - val_mse: 423.5871 - learning_rate: 1.2500e-04\n",
            "Epoch 46/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 10.0195 - mse: 468.3711 - val_loss: 5.9827 - val_mse: 419.8616 - learning_rate: 1.2500e-04\n",
            "Epoch 47/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 9.9858 - mse: 463.9552 - val_loss: 6.0078 - val_mse: 420.6326 - learning_rate: 1.2500e-04\n",
            "Epoch 48/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 9.9406 - mse: 458.0357 - val_loss: 5.9747 - val_mse: 424.0048 - learning_rate: 1.2500e-04\n",
            "Epoch 49/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 9.9320 - mse: 452.4503 - val_loss: 5.9711 - val_mse: 421.8097 - learning_rate: 1.2500e-04\n",
            "Epoch 50/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 32ms/step - loss: 9.9413 - mse: 455.9354 - val_loss: 6.0659 - val_mse: 430.0019 - learning_rate: 1.2500e-04\n",
            "Epoch 51/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 32ms/step - loss: 9.9018 - mse: 449.6242 - val_loss: 5.9874 - val_mse: 426.1490 - learning_rate: 1.2500e-04\n",
            "Epoch 52/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 32ms/step - loss: 9.9246 - mse: 453.1765 - val_loss: 5.9658 - val_mse: 421.5300 - learning_rate: 1.2500e-04\n",
            "Epoch 53/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 31ms/step - loss: 9.9049 - mse: 448.2219 - val_loss: 5.9789 - val_mse: 419.9904 - learning_rate: 1.2500e-04\n",
            "Epoch 54/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 9.9067 - mse: 449.6169 - val_loss: 5.9857 - val_mse: 425.1302 - learning_rate: 1.2500e-04\n",
            "Epoch 55/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 9.9022 - mse: 449.8426 - val_loss: 5.9711 - val_mse: 421.2111 - learning_rate: 1.2500e-04\n",
            "Epoch 56/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 9.8823 - mse: 447.3371 - val_loss: 5.9729 - val_mse: 420.3278 - learning_rate: 1.2500e-04\n",
            "Epoch 57/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 9.8683 - mse: 448.4740 - val_loss: 5.9381 - val_mse: 418.0098 - learning_rate: 1.2500e-04\n",
            "Epoch 58/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 9.8979 - mse: 453.4553 - val_loss: 6.1063 - val_mse: 437.1987 - learning_rate: 1.2500e-04\n",
            "Epoch 59/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 9.8862 - mse: 444.7780 - val_loss: 6.0523 - val_mse: 426.5766 - learning_rate: 1.2500e-04\n",
            "Epoch 60/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 9.8480 - mse: 441.3489 - val_loss: 6.3981 - val_mse: 442.4633 - learning_rate: 1.2500e-04\n",
            "  Chosen mix: MSE=0.20, MAX=0.80 | threshold=130.266708 | val Sens=0.627, Spec=0.978\n",
            "  TAIL-ONLY → Sens=0.668, Spec=0.976, FPR=0.024, POD=0.930, Tim=0.215\n",
            "\n",
            "=== Signal 6 (LSTM AE; IF-driven splits; tail-only metrics) ===\n",
            "  Using splits (from IF files): 300 train, 100 val, 100 test\n",
            "  Train windows: 651300 from first 6 years\n",
            "  Val windows (tail, full coverage): 34300  positives: 857 (2.5%)\n",
            "  Test windows (tail, full coverage): 34300\n",
            "Epoch 1/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 31ms/step - loss: 2.0382 - mse: 20.2972 - val_loss: 1.3341 - val_mse: 10.1599 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 1.3052 - mse: 9.1811 - val_loss: 1.1471 - val_mse: 7.5388 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 1.1833 - mse: 7.5034 - val_loss: 1.0767 - val_mse: 6.9277 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 1.1508 - mse: 7.1824 - val_loss: 1.0500 - val_mse: 6.5802 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 1.1245 - mse: 6.9390 - val_loss: 1.0698 - val_mse: 6.6906 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 1.1088 - mse: 6.6708 - val_loss: 1.0291 - val_mse: 6.3460 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 1.0958 - mse: 6.5303 - val_loss: 0.9877 - val_mse: 6.0472 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 1.0765 - mse: 6.3644 - val_loss: 0.9620 - val_mse: 5.8796 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 1.0550 - mse: 6.1404 - val_loss: 0.9684 - val_mse: 5.9109 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 1.1328 - mse: 6.9760 - val_loss: 0.9646 - val_mse: 5.8314 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 1.0443 - mse: 5.9944 - val_loss: 0.9480 - val_mse: 5.7769 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 1.0478 - mse: 6.0766 - val_loss: 0.9957 - val_mse: 6.4358 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 1.0783 - mse: 6.4113 - val_loss: 0.9585 - val_mse: 5.8104 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 1.0386 - mse: 5.9504 - val_loss: 0.9892 - val_mse: 6.6045 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 1.0229 - mse: 5.8361 - val_loss: 0.9393 - val_mse: 5.6047 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 1.0373 - mse: 5.9654 - val_loss: 0.9210 - val_mse: 5.5595 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 1.0349 - mse: 5.9529 - val_loss: 1.1992 - val_mse: 8.0927 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 1.0151 - mse: 5.7264 - val_loss: 0.9470 - val_mse: 5.7340 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 1.0439 - mse: 6.0428 - val_loss: 0.9238 - val_mse: 5.4946 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 0.9936 - mse: 5.5332 - val_loss: 0.8971 - val_mse: 5.3242 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 1.0034 - mse: 5.6583 - val_loss: 0.9286 - val_mse: 5.5339 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 1.0039 - mse: 5.6721 - val_loss: 0.9591 - val_mse: 5.8564 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 1.0142 - mse: 5.7996 - val_loss: 0.8935 - val_mse: 5.3302 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 0.9765 - mse: 5.3795 - val_loss: 0.9094 - val_mse: 5.3485 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 0.9873 - mse: 5.4943 - val_loss: 0.8857 - val_mse: 5.2524 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 0.9887 - mse: 5.5039 - val_loss: 0.9062 - val_mse: 5.4223 - learning_rate: 0.0010\n",
            "Epoch 27/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 0.9771 - mse: 5.3637 - val_loss: 0.8745 - val_mse: 5.1557 - learning_rate: 0.0010\n",
            "Epoch 28/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 0.9715 - mse: 5.3531 - val_loss: 0.9054 - val_mse: 5.3253 - learning_rate: 0.0010\n",
            "Epoch 29/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 31ms/step - loss: 0.9768 - mse: 5.4485 - val_loss: 0.9098 - val_mse: 5.3701 - learning_rate: 0.0010\n",
            "Epoch 30/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 0.9781 - mse: 5.4048 - val_loss: 0.8657 - val_mse: 5.0924 - learning_rate: 0.0010\n",
            "Epoch 31/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 0.9574 - mse: 5.2209 - val_loss: 1.0148 - val_mse: 6.2942 - learning_rate: 0.0010\n",
            "Epoch 32/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 0.9827 - mse: 5.4695 - val_loss: 0.8713 - val_mse: 5.1049 - learning_rate: 0.0010\n",
            "Epoch 33/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 0.9508 - mse: 5.1582 - val_loss: 0.8622 - val_mse: 5.1067 - learning_rate: 0.0010\n",
            "Epoch 34/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 0.9555 - mse: 5.2269 - val_loss: 0.8832 - val_mse: 5.2126 - learning_rate: 0.0010\n",
            "Epoch 35/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 0.9560 - mse: 5.2180 - val_loss: 0.8554 - val_mse: 5.0564 - learning_rate: 0.0010\n",
            "Epoch 36/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 30ms/step - loss: 0.9539 - mse: 5.2203 - val_loss: 0.8501 - val_mse: 5.0163 - learning_rate: 0.0010\n",
            "Epoch 37/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 30ms/step - loss: 0.9499 - mse: 5.1576 - val_loss: 0.9408 - val_mse: 5.7074 - learning_rate: 0.0010\n",
            "Epoch 38/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 0.9590 - mse: 5.2380 - val_loss: 0.9021 - val_mse: 5.3913 - learning_rate: 0.0010\n",
            "Epoch 39/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 0.9470 - mse: 5.1479 - val_loss: 0.8540 - val_mse: 5.0707 - learning_rate: 0.0010\n",
            "Epoch 40/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 30ms/step - loss: 0.9592 - mse: 5.2476 - val_loss: 0.8841 - val_mse: 5.2117 - learning_rate: 0.0010\n",
            "Epoch 41/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 29ms/step - loss: 0.9550 - mse: 5.1973 - val_loss: 1.1536 - val_mse: 7.6452 - learning_rate: 0.0010\n",
            "Epoch 42/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 0.9568 - mse: 5.2047 - val_loss: 0.8454 - val_mse: 4.9437 - learning_rate: 5.0000e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 0.9204 - mse: 4.8717 - val_loss: 0.8354 - val_mse: 4.9242 - learning_rate: 5.0000e-04\n",
            "Epoch 44/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 0.9177 - mse: 4.8756 - val_loss: 0.8264 - val_mse: 4.7849 - learning_rate: 5.0000e-04\n",
            "Epoch 45/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 0.9080 - mse: 4.7808 - val_loss: 0.8240 - val_mse: 4.7781 - learning_rate: 5.0000e-04\n",
            "Epoch 46/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 0.9072 - mse: 4.7936 - val_loss: 0.8261 - val_mse: 4.7624 - learning_rate: 5.0000e-04\n",
            "Epoch 47/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 0.9013 - mse: 4.7205 - val_loss: 0.8259 - val_mse: 4.7630 - learning_rate: 5.0000e-04\n",
            "Epoch 48/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 0.9060 - mse: 4.7554 - val_loss: 0.8270 - val_mse: 4.7643 - learning_rate: 5.0000e-04\n",
            "Epoch 49/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 0.9085 - mse: 4.7878 - val_loss: 0.8309 - val_mse: 4.7865 - learning_rate: 5.0000e-04\n",
            "Epoch 50/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 0.9171 - mse: 4.8492 - val_loss: 0.8270 - val_mse: 4.7453 - learning_rate: 5.0000e-04\n",
            "Epoch 51/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 0.9018 - mse: 4.7194 - val_loss: 0.8062 - val_mse: 4.6447 - learning_rate: 2.5000e-04\n",
            "Epoch 52/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 0.8869 - mse: 4.5981 - val_loss: 0.8052 - val_mse: 4.6332 - learning_rate: 2.5000e-04\n",
            "Epoch 53/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 0.8818 - mse: 4.5601 - val_loss: 0.8050 - val_mse: 4.5774 - learning_rate: 2.5000e-04\n",
            "Epoch 54/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 0.8851 - mse: 4.5958 - val_loss: 0.7994 - val_mse: 4.5787 - learning_rate: 2.5000e-04\n",
            "Epoch 55/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 0.8772 - mse: 4.5230 - val_loss: 0.7970 - val_mse: 4.5789 - learning_rate: 2.5000e-04\n",
            "Epoch 56/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 31ms/step - loss: 0.8782 - mse: 4.5424 - val_loss: 0.7994 - val_mse: 4.5674 - learning_rate: 2.5000e-04\n",
            "Epoch 57/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 0.8814 - mse: 4.5645 - val_loss: 0.7994 - val_mse: 4.5533 - learning_rate: 2.5000e-04\n",
            "Epoch 58/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 31ms/step - loss: 0.8961 - mse: 4.6949 - val_loss: 0.8010 - val_mse: 4.6181 - learning_rate: 2.5000e-04\n",
            "Epoch 59/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 0.8820 - mse: 4.5581 - val_loss: 0.7946 - val_mse: 4.5461 - learning_rate: 2.5000e-04\n",
            "Epoch 60/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 0.8746 - mse: 4.4980 - val_loss: 0.7928 - val_mse: 4.5326 - learning_rate: 2.5000e-04\n",
            "  Chosen mix: MSE=1.00, MAX=0.00 | threshold=17.859865 | val Sens=0.370, Spec=0.979\n",
            "  TAIL-ONLY → Sens=0.384, Spec=0.979, FPR=0.021, POD=0.460, Tim=0.658\n",
            "\n",
            "=== Signal 7 (LSTM AE; IF-driven splits; tail-only metrics) ===\n",
            "  Using splits (from IF files): 300 train, 100 val, 100 test\n",
            "  Train windows: 651300 from first 6 years\n",
            "  Val windows (tail, full coverage): 34300  positives: 1569 (4.6%)\n",
            "  Test windows (tail, full coverage): 34300\n",
            "Epoch 1/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 31ms/step - loss: 10.2833 - mse: 644.8156 - val_loss: 6.6959 - val_mse: 467.7307 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 3.0420 - mse: 65.7655 - val_loss: 6.8081 - val_mse: 470.2956 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 2.7872 - mse: 57.1919 - val_loss: 6.9963 - val_mse: 487.6866 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 31ms/step - loss: 2.7287 - mse: 55.7569 - val_loss: 7.0168 - val_mse: 476.5617 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 2.7803 - mse: 61.3562 - val_loss: 6.9118 - val_mse: 492.5991 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 2.7271 - mse: 57.9693 - val_loss: 6.9779 - val_mse: 479.5689 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 2.5512 - mse: 47.3729 - val_loss: 6.8848 - val_mse: 477.2346 - learning_rate: 5.0000e-04\n",
            "Epoch 8/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 2.5267 - mse: 47.2245 - val_loss: 5.6037 - val_mse: 450.5837 - learning_rate: 5.0000e-04\n",
            "Epoch 9/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 31ms/step - loss: 2.3255 - mse: 49.1015 - val_loss: 5.6663 - val_mse: 454.9593 - learning_rate: 5.0000e-04\n",
            "Epoch 10/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 2.2281 - mse: 44.6880 - val_loss: 5.3481 - val_mse: 449.7904 - learning_rate: 5.0000e-04\n",
            "Epoch 11/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 2.2156 - mse: 43.9979 - val_loss: 5.1956 - val_mse: 445.2751 - learning_rate: 5.0000e-04\n",
            "Epoch 12/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 2.1328 - mse: 42.2234 - val_loss: 5.0437 - val_mse: 443.5351 - learning_rate: 5.0000e-04\n",
            "Epoch 13/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 2.1051 - mse: 40.3959 - val_loss: 5.1218 - val_mse: 442.1481 - learning_rate: 5.0000e-04\n",
            "Epoch 14/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 2.0892 - mse: 39.7262 - val_loss: 5.0082 - val_mse: 425.4620 - learning_rate: 5.0000e-04\n",
            "Epoch 15/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 2.0761 - mse: 38.8306 - val_loss: 5.0822 - val_mse: 458.5792 - learning_rate: 5.0000e-04\n",
            "Epoch 16/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 2.0672 - mse: 38.0099 - val_loss: 5.5887 - val_mse: 483.1516 - learning_rate: 5.0000e-04\n",
            "Epoch 17/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 2.0987 - mse: 38.8282 - val_loss: 5.1962 - val_mse: 432.7473 - learning_rate: 5.0000e-04\n",
            "Epoch 18/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 2.0899 - mse: 40.3111 - val_loss: 5.1739 - val_mse: 452.9788 - learning_rate: 5.0000e-04\n",
            "Epoch 19/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 2.1207 - mse: 39.6337 - val_loss: 5.1092 - val_mse: 465.7226 - learning_rate: 5.0000e-04\n",
            "Epoch 20/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 1.9749 - mse: 34.5486 - val_loss: 5.0152 - val_mse: 449.6845 - learning_rate: 2.5000e-04\n",
            "Epoch 21/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 1.9617 - mse: 34.7798 - val_loss: 4.9656 - val_mse: 454.5857 - learning_rate: 2.5000e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 31ms/step - loss: 1.9637 - mse: 34.1712 - val_loss: 4.8671 - val_mse: 449.4778 - learning_rate: 2.5000e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 1.9423 - mse: 33.3323 - val_loss: 4.8345 - val_mse: 446.7836 - learning_rate: 2.5000e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 1.9199 - mse: 33.2283 - val_loss: 4.8167 - val_mse: 440.8593 - learning_rate: 2.5000e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 1.9206 - mse: 33.4907 - val_loss: 4.8413 - val_mse: 447.0609 - learning_rate: 2.5000e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 31ms/step - loss: 1.8870 - mse: 31.0487 - val_loss: 6.8859 - val_mse: 452.9767 - learning_rate: 2.5000e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 31ms/step - loss: 1.9007 - mse: 31.3303 - val_loss: 4.8055 - val_mse: 441.8239 - learning_rate: 2.5000e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 1.8627 - mse: 27.9844 - val_loss: 4.6631 - val_mse: 422.2798 - learning_rate: 2.5000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 1.8102 - mse: 25.1104 - val_loss: 4.6722 - val_mse: 426.6425 - learning_rate: 2.5000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 1.8017 - mse: 25.6526 - val_loss: 4.6766 - val_mse: 430.9605 - learning_rate: 2.5000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 1.7939 - mse: 25.2011 - val_loss: 4.7049 - val_mse: 439.5981 - learning_rate: 2.5000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 1.8020 - mse: 25.7606 - val_loss: 4.5723 - val_mse: 420.7799 - learning_rate: 2.5000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 1.7832 - mse: 24.4400 - val_loss: 4.8643 - val_mse: 427.0532 - learning_rate: 2.5000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 1.8002 - mse: 24.8666 - val_loss: 4.4964 - val_mse: 409.1227 - learning_rate: 2.5000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 1.7633 - mse: 23.3158 - val_loss: 4.5550 - val_mse: 417.1446 - learning_rate: 2.5000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 1.7631 - mse: 24.0221 - val_loss: 4.5779 - val_mse: 423.4983 - learning_rate: 2.5000e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 1.7756 - mse: 24.9486 - val_loss: 4.5187 - val_mse: 424.8409 - learning_rate: 2.5000e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 1.7569 - mse: 24.2127 - val_loss: 4.5232 - val_mse: 421.3930 - learning_rate: 2.5000e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 1.7423 - mse: 22.9481 - val_loss: 4.5923 - val_mse: 429.2330 - learning_rate: 2.5000e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 1.7285 - mse: 23.0307 - val_loss: 4.5088 - val_mse: 418.2050 - learning_rate: 1.2500e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 1.7057 - mse: 21.2854 - val_loss: 4.4869 - val_mse: 421.3781 - learning_rate: 1.2500e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 1.7065 - mse: 21.4554 - val_loss: 4.5364 - val_mse: 427.5955 - learning_rate: 1.2500e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 1.6958 - mse: 20.8978 - val_loss: 4.4757 - val_mse: 415.9777 - learning_rate: 1.2500e-04\n",
            "Epoch 44/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 1.7038 - mse: 20.7771 - val_loss: 4.6302 - val_mse: 411.4877 - learning_rate: 1.2500e-04\n",
            "Epoch 45/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 1.7060 - mse: 20.7424 - val_loss: 4.4905 - val_mse: 422.5731 - learning_rate: 1.2500e-04\n",
            "Epoch 46/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 1.6882 - mse: 19.7276 - val_loss: 4.4742 - val_mse: 421.4484 - learning_rate: 1.2500e-04\n",
            "Epoch 47/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 1.6867 - mse: 20.2038 - val_loss: 4.5092 - val_mse: 418.2260 - learning_rate: 1.2500e-04\n",
            "Epoch 48/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 1.6807 - mse: 20.0031 - val_loss: 4.5195 - val_mse: 426.7505 - learning_rate: 1.2500e-04\n",
            "Epoch 49/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 1.6767 - mse: 19.9040 - val_loss: 4.5119 - val_mse: 425.7244 - learning_rate: 1.2500e-04\n",
            "Epoch 50/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 1.6878 - mse: 20.9071 - val_loss: 4.5651 - val_mse: 426.6753 - learning_rate: 1.2500e-04\n",
            "Epoch 51/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 1.6746 - mse: 20.0501 - val_loss: 4.4697 - val_mse: 424.7633 - learning_rate: 1.2500e-04\n",
            "Epoch 52/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 1.6706 - mse: 19.4840 - val_loss: 4.4302 - val_mse: 413.8549 - learning_rate: 1.2500e-04\n",
            "Epoch 53/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 1.6780 - mse: 20.2685 - val_loss: 4.4554 - val_mse: 416.9825 - learning_rate: 1.2500e-04\n",
            "Epoch 54/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 31ms/step - loss: 1.6844 - mse: 20.0606 - val_loss: 4.4649 - val_mse: 418.6276 - learning_rate: 1.2500e-04\n",
            "Epoch 55/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 31ms/step - loss: 1.6667 - mse: 19.3051 - val_loss: 4.5801 - val_mse: 422.5017 - learning_rate: 1.2500e-04\n",
            "Epoch 56/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 1.6675 - mse: 19.8074 - val_loss: 4.4427 - val_mse: 415.3030 - learning_rate: 1.2500e-04\n",
            "Epoch 57/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 1.6605 - mse: 19.5872 - val_loss: 4.4336 - val_mse: 411.8562 - learning_rate: 1.2500e-04\n",
            "Epoch 58/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 31ms/step - loss: 1.6594 - mse: 19.7343 - val_loss: 4.3978 - val_mse: 408.9506 - learning_rate: 6.2500e-05\n",
            "Epoch 59/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 1.6569 - mse: 19.5701 - val_loss: 4.3982 - val_mse: 411.0161 - learning_rate: 6.2500e-05\n",
            "Epoch 60/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 31ms/step - loss: 1.6506 - mse: 19.4378 - val_loss: 4.3141 - val_mse: 408.2921 - learning_rate: 6.2500e-05\n",
            "  Chosen mix: MSE=1.00, MAX=0.00 | threshold=533.245789 | val Sens=0.849, Spec=0.978\n",
            "  TAIL-ONLY → Sens=0.846, Spec=0.979, FPR=0.021, POD=1.000, Tim=0.084\n",
            "\n",
            "=== Signal 8 (LSTM AE; IF-driven splits; tail-only metrics) ===\n",
            "  Using splits (from IF files): 300 train, 100 val, 100 test\n",
            "  Train windows: 651300 from first 6 years\n",
            "  Val windows (tail, full coverage): 34300  positives: 1127 (3.3%)\n",
            "  Test windows (tail, full coverage): 34300\n",
            "Epoch 1/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 31ms/step - loss: 7.9413 - mse: 314.2830 - val_loss: 4.0585 - val_mse: 96.5728 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 31ms/step - loss: 4.8121 - mse: 113.5071 - val_loss: 4.4089 - val_mse: 99.4694 - learning_rate: 0.0010\n",
            "Epoch 3/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 4.4113 - mse: 95.7885 - val_loss: 3.3908 - val_mse: 59.2781 - learning_rate: 0.0010\n",
            "Epoch 4/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 4.3028 - mse: 90.6895 - val_loss: 3.5423 - val_mse: 61.9236 - learning_rate: 0.0010\n",
            "Epoch 5/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 4.0414 - mse: 74.4981 - val_loss: 3.2318 - val_mse: 49.6991 - learning_rate: 0.0010\n",
            "Epoch 6/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 3.8565 - mse: 62.8118 - val_loss: 2.9576 - val_mse: 34.4983 - learning_rate: 0.0010\n",
            "Epoch 7/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 31ms/step - loss: 3.6139 - mse: 52.5323 - val_loss: 2.8567 - val_mse: 32.0089 - learning_rate: 0.0010\n",
            "Epoch 8/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 3.5034 - mse: 49.5778 - val_loss: 2.8001 - val_mse: 31.0634 - learning_rate: 0.0010\n",
            "Epoch 9/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 3.4855 - mse: 48.0441 - val_loss: 2.8070 - val_mse: 31.4662 - learning_rate: 0.0010\n",
            "Epoch 10/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 3.5985 - mse: 53.4045 - val_loss: 2.7904 - val_mse: 31.3438 - learning_rate: 0.0010\n",
            "Epoch 11/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 3.4067 - mse: 46.1057 - val_loss: 3.2181 - val_mse: 45.5659 - learning_rate: 0.0010\n",
            "Epoch 12/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 3.5994 - mse: 53.7773 - val_loss: 2.8062 - val_mse: 31.4765 - learning_rate: 0.0010\n",
            "Epoch 13/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 31ms/step - loss: 3.3553 - mse: 44.0195 - val_loss: 2.7689 - val_mse: 30.4442 - learning_rate: 0.0010\n",
            "Epoch 14/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 3.2997 - mse: 42.3225 - val_loss: 3.0304 - val_mse: 35.1783 - learning_rate: 0.0010\n",
            "Epoch 15/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 3.4474 - mse: 48.5295 - val_loss: 2.9788 - val_mse: 38.0551 - learning_rate: 0.0010\n",
            "Epoch 16/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 31ms/step - loss: 3.4129 - mse: 47.0511 - val_loss: 2.8378 - val_mse: 32.0064 - learning_rate: 0.0010\n",
            "Epoch 17/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 31ms/step - loss: 3.4073 - mse: 47.1143 - val_loss: 2.9302 - val_mse: 35.1666 - learning_rate: 0.0010\n",
            "Epoch 18/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 3.2907 - mse: 43.3920 - val_loss: 2.6605 - val_mse: 29.1979 - learning_rate: 0.0010\n",
            "Epoch 19/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 3.2587 - mse: 41.4759 - val_loss: 2.7210 - val_mse: 30.7062 - learning_rate: 0.0010\n",
            "Epoch 20/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 31ms/step - loss: 3.3261 - mse: 43.4637 - val_loss: 2.7549 - val_mse: 30.9210 - learning_rate: 0.0010\n",
            "Epoch 21/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 3.2212 - mse: 40.1676 - val_loss: 2.9137 - val_mse: 34.9720 - learning_rate: 0.0010\n",
            "Epoch 22/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 3.2352 - mse: 41.8797 - val_loss: 2.6415 - val_mse: 28.7765 - learning_rate: 0.0010\n",
            "Epoch 23/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 3.2056 - mse: 39.9947 - val_loss: 2.6726 - val_mse: 29.5336 - learning_rate: 0.0010\n",
            "Epoch 24/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 3.1720 - mse: 38.3406 - val_loss: 2.6608 - val_mse: 29.3473 - learning_rate: 0.0010\n",
            "Epoch 25/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 3.1714 - mse: 39.1055 - val_loss: 2.6494 - val_mse: 28.8060 - learning_rate: 0.0010\n",
            "Epoch 26/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 3.1830 - mse: 39.6455 - val_loss: 2.7162 - val_mse: 30.0577 - learning_rate: 0.0010\n",
            "Epoch 27/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 3.1554 - mse: 37.9523 - val_loss: 2.6683 - val_mse: 29.1775 - learning_rate: 0.0010\n",
            "Epoch 28/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 3.1052 - mse: 36.3333 - val_loss: 2.6024 - val_mse: 28.3483 - learning_rate: 5.0000e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 3.0480 - mse: 35.1411 - val_loss: 2.5913 - val_mse: 27.9730 - learning_rate: 5.0000e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 3.0639 - mse: 36.0538 - val_loss: 2.5927 - val_mse: 28.0543 - learning_rate: 5.0000e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 3.0252 - mse: 34.2407 - val_loss: 2.6176 - val_mse: 28.2698 - learning_rate: 5.0000e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 3.0261 - mse: 34.5195 - val_loss: 2.5840 - val_mse: 28.0402 - learning_rate: 5.0000e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 31ms/step - loss: 2.9883 - mse: 32.9530 - val_loss: 2.5730 - val_mse: 27.7128 - learning_rate: 5.0000e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 31ms/step - loss: 2.9884 - mse: 33.3446 - val_loss: 2.5658 - val_mse: 27.7446 - learning_rate: 5.0000e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 2.9851 - mse: 33.3642 - val_loss: 2.5662 - val_mse: 27.7564 - learning_rate: 5.0000e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 2.9755 - mse: 32.9428 - val_loss: 2.5843 - val_mse: 28.0512 - learning_rate: 5.0000e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 2.9817 - mse: 33.3645 - val_loss: 2.5445 - val_mse: 27.4128 - learning_rate: 5.0000e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 2.9791 - mse: 33.4266 - val_loss: 2.6966 - val_mse: 30.0218 - learning_rate: 5.0000e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 2.9894 - mse: 34.2044 - val_loss: 2.5455 - val_mse: 27.4213 - learning_rate: 5.0000e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 2.9857 - mse: 34.2647 - val_loss: 2.5817 - val_mse: 27.9253 - learning_rate: 5.0000e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 2.9827 - mse: 33.3045 - val_loss: 2.5284 - val_mse: 27.1777 - learning_rate: 5.0000e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 31ms/step - loss: 2.9764 - mse: 33.5650 - val_loss: 2.5266 - val_mse: 27.1513 - learning_rate: 5.0000e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 31ms/step - loss: 2.9667 - mse: 33.4865 - val_loss: 2.5120 - val_mse: 26.9672 - learning_rate: 5.0000e-04\n",
            "Epoch 44/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 30ms/step - loss: 2.9563 - mse: 33.0324 - val_loss: 2.5184 - val_mse: 26.9613 - learning_rate: 5.0000e-04\n",
            "Epoch 45/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 2.9311 - mse: 32.1690 - val_loss: 2.5351 - val_mse: 27.2364 - learning_rate: 5.0000e-04\n",
            "Epoch 46/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 2.9474 - mse: 33.1630 - val_loss: 2.5003 - val_mse: 26.7992 - learning_rate: 5.0000e-04\n",
            "Epoch 47/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 2.9483 - mse: 33.1990 - val_loss: 2.4799 - val_mse: 26.5943 - learning_rate: 5.0000e-04\n",
            "Epoch 48/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 3.0080 - mse: 35.2005 - val_loss: 2.5096 - val_mse: 26.8728 - learning_rate: 5.0000e-04\n",
            "Epoch 49/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 2.9335 - mse: 32.3372 - val_loss: 2.4906 - val_mse: 26.5776 - learning_rate: 5.0000e-04\n",
            "Epoch 50/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 2.9267 - mse: 32.1140 - val_loss: 2.5024 - val_mse: 26.8916 - learning_rate: 5.0000e-04\n",
            "Epoch 51/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 31ms/step - loss: 2.9208 - mse: 32.0541 - val_loss: 2.4553 - val_mse: 26.1049 - learning_rate: 5.0000e-04\n",
            "Epoch 52/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 30ms/step - loss: 2.9247 - mse: 32.5938 - val_loss: 2.5021 - val_mse: 27.0672 - learning_rate: 5.0000e-04\n",
            "Epoch 53/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 2.9535 - mse: 34.3564 - val_loss: 2.4998 - val_mse: 26.9875 - learning_rate: 5.0000e-04\n",
            "Epoch 54/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 2.9239 - mse: 32.1577 - val_loss: 2.4933 - val_mse: 26.9760 - learning_rate: 5.0000e-04\n",
            "Epoch 55/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 2.9689 - mse: 34.1988 - val_loss: 2.4458 - val_mse: 26.0600 - learning_rate: 5.0000e-04\n",
            "Epoch 56/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 2.9265 - mse: 32.2894 - val_loss: 2.4882 - val_mse: 26.8286 - learning_rate: 5.0000e-04\n",
            "Epoch 57/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 30ms/step - loss: 2.9020 - mse: 31.5544 - val_loss: 2.4914 - val_mse: 26.6479 - learning_rate: 5.0000e-04\n",
            "Epoch 58/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - loss: 2.9041 - mse: 32.3648 - val_loss: 2.5121 - val_mse: 27.2216 - learning_rate: 5.0000e-04\n",
            "Epoch 59/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 30ms/step - loss: 2.8947 - mse: 31.5078 - val_loss: 2.4202 - val_mse: 25.8085 - learning_rate: 5.0000e-04\n",
            "Epoch 60/60\n",
            "\u001b[1m5089/5089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 29ms/step - loss: 2.8822 - mse: 31.4517 - val_loss: 2.4087 - val_mse: 25.4930 - learning_rate: 5.0000e-04\n",
            "  Chosen mix: MSE=0.70, MAX=0.30 | threshold=60.775856 | val Sens=0.469, Spec=0.975\n",
            "  TAIL-ONLY → Sens=0.476, Spec=0.975, FPR=0.025, POD=0.620, Tim=0.503\n",
            "\n",
            "=== LSTM Autoencoder — Tail-only Comparator Metrics (by signal) ===\n",
            "        sensitivity  specificity       fpr   pod  timeliness   threshold  \\\n",
            "signal                                                                     \n",
            "5          0.667927     0.975942  0.024058  0.93    0.215407  130.266708   \n",
            "6          0.384000     0.978519  0.021481  0.46    0.657910   17.859865   \n",
            "7          0.846405     0.978639  0.021361  1.00    0.084277  533.245789   \n",
            "8          0.475806     0.974867  0.025133  0.62    0.502750   60.775856   \n",
            "\n",
            "        mix_mse  mix_max  nsim_test  \n",
            "signal                               \n",
            "5           0.2      0.8        100  \n",
            "6           1.0      0.0        100  \n",
            "7           1.0      0.0        100  \n",
            "8           0.7      0.3        100  \n",
            "\n",
            "=== Means ===\n",
            "sensitivity    0.593535\n",
            "specificity    0.976992\n",
            "fpr            0.023008\n",
            "pod            0.752500\n",
            "timeliness     0.365086\n",
            "dtype: float64\n",
            "\n",
            "Saved: LSTM_AE_tail_only_metrics.csv\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "LSTM Autoencoder — Tail-only metrics, using IsolationForest per-sim splits\n",
        "- Uses IsolationForest_per_sim_val.csv and IsolationForest_per_sim_test.csv to define splits.\n",
        "  If a 'split' column exists with values {train,val,test}, those are used directly\n",
        "  (supports 60/20/20 from IF). If absent, rows from the *_val.csv are 'val' and\n",
        "  *_test.csv are 'test', and TRAIN = remaining sims.\n",
        "- Unsupervised training on first 6 years from TRAIN sims\n",
        "- Validation tuner (mix + threshold), optional guard-rail threshold (train 95th pct for chosen mix)\n",
        "- Evaluates tail-only comparator metrics (last 49 weeks) with FULL per-day coverage (no front padding)\n",
        "\n",
        "Files written:\n",
        "  - LSTM_AE_tail_only_metrics.csv\n",
        "  - (optional confirmation) lstm_val_indices_sig{S}.csv, lstm_test_indices_sig{S}.csv\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "DATA_DIR      = \"\"   # <-- set folder with simulated_totals_sig{S}.csv & simulated_outbreaks_sig{S}.csv\n",
        "VAL_SPLIT_CSV = \"IsolationForest_big_medium_per_sim_val.csv\"\n",
        "TEST_SPLIT_CSV= \"IsolationForest_big_medium_per_sim_test.csv\"\n",
        "\n",
        "SIGNALS       = list(range(1, 5))\n",
        "DAYS_PER_YEAR = 364\n",
        "SEQ, STRIDE   = 14, 1\n",
        "TRAIN_YEARS   = 6\n",
        "TRAIN_DAYS    = TRAIN_YEARS * DAYS_PER_YEAR\n",
        "TAIL_DAYS     = 49 * 7   # 343 (last 49 weeks)\n",
        "\n",
        "# Training\n",
        "EPOCHS        = 60\n",
        "BATCH_SIZE    = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "PATIENCE      = 7\n",
        "MIN_EPOCHS    = 20\n",
        "RNG_STATE     = 42\n",
        "\n",
        "# Validation tuning\n",
        "SPEC_TARGET       = 0.97\n",
        "TUNE_WEIGHT_SENS  = 2.0\n",
        "TUNE_WEIGHT_SPEC  = 3.0\n",
        "\n",
        "MIX_GRID = (\n",
        "    (1.00, 0.00), (0.95, 0.05), (0.90, 0.10),\n",
        "    (0.80, 0.20), (0.70, 0.30), (0.60, 0.40),\n",
        "    (0.40, 0.60), (0.30, 0.70), (0.20, 0.80),\n",
        "    (0.10, 0.90), (0.05, 0.95), (0.00, 1.00),\n",
        ")\n",
        "ANOMALY_MIX_MSE   = 0.8\n",
        "ANOMALY_MIX_MAX   = 0.2\n",
        "\n",
        "np.random.seed(RNG_STATE)\n",
        "tf.random.set_seed(RNG_STATE)\n",
        "\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices(\"GPU\"))\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    except Exception as e:\n",
        "        print(\"Could not set memory growth:\", e)\n",
        "\n",
        "# ========== SPLIT HELPERS ==========\n",
        "SIM_RE = re.compile(r\"^sig(?P<sig>\\d+)_sim(?P<idx>\\d+)$\")\n",
        "\n",
        "def parse_sim_name(sim_name):\n",
        "    m = SIM_RE.match(sim_name)\n",
        "    if not m:\n",
        "        return None, None\n",
        "    return int(m.group(\"sig\")), int(m.group(\"idx\"))\n",
        "\n",
        "def load_if_splits(val_csv, test_csv):\n",
        "    \"\"\"\n",
        "    Return dict: signal -> {'train': set(sim_names), 'val': set(sim_names), 'test': set(sim_names)}.\n",
        "\n",
        "    If CSVs include a 'split' column with values in {train,val,test}, those are used directly\n",
        "    (supports a single large-file style or explicit 60/20/20 lists stored across the two CSVs).\n",
        "    If 'split' is absent, rows from `val_csv` are treated as 'val', rows from `test_csv` as 'test',\n",
        "    and TRAIN will be derived as the leftover sims present in the data.\n",
        "    \"\"\"\n",
        "    def _load(path, default_split):\n",
        "        df = pd.read_csv(path)\n",
        "        if \"signal\" not in df.columns or \"sim\" not in df.columns:\n",
        "            raise ValueError(f\"{path} must contain columns ['signal','sim', ...]\")\n",
        "        split_col = df[\"split\"].astype(str).str.lower() if \"split\" in df.columns else pd.Series([default_split]*len(df))\n",
        "        return pd.DataFrame({\n",
        "            \"signal\": df[\"signal\"].astype(int),\n",
        "            \"sim\": df[\"sim\"].astype(str),\n",
        "            \"split\": split_col\n",
        "        })\n",
        "\n",
        "    dfv = _load(val_csv,  default_split=\"val\")\n",
        "    dft = _load(test_csv, default_split=\"test\")\n",
        "    df  = pd.concat([dfv, dft], ignore_index=True)\n",
        "\n",
        "    by_signal = {}\n",
        "    for _, row in df.iterrows():\n",
        "        sig = int(row[\"signal\"]); sim = str(row[\"sim\"]); split = str(row[\"split\"]).lower()\n",
        "        d = by_signal.setdefault(sig, {\"train\": set(), \"val\": set(), \"test\": set()})\n",
        "        if split in {\"train\", \"val\", \"test\"}:\n",
        "            d[split].add(sim)\n",
        "    return by_signal\n",
        "\n",
        "# ========== DATA HELPERS ==========\n",
        "def load_signal(sig):\n",
        "    x_path = os.path.join(DATA_DIR, f\"simulated_totals_sig{sig}.csv\")\n",
        "    y_path = os.path.join(DATA_DIR, f\"simulated_outbreaks_sig{sig}.csv\")\n",
        "    if not (os.path.exists(x_path) and os.path.exists(y_path)):\n",
        "        raise FileNotFoundError(f\"Missing CSVs for signal {sig}: {x_path}, {y_path}\")\n",
        "    X = pd.read_csv(x_path)\n",
        "    Y = pd.read_csv(y_path)\n",
        "    for c in list(X.columns):\n",
        "        if c.lower() in (\"date\", \"ds\", \"timestamp\"):\n",
        "            X = X.drop(columns=[c])\n",
        "            if c in Y.columns:\n",
        "                Y = Y.drop(columns=[c])\n",
        "            break\n",
        "    X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    Y = (Y.apply(pd.to_numeric, errors=\"coerce\") > 0).astype(int)\n",
        "    return X, Y\n",
        "\n",
        "def make_seq_labels(series, labels, seq_len=SEQ, stride=STRIDE):\n",
        "    X, Y = [], []\n",
        "    for i in range(0, len(series) - seq_len + 1, stride):\n",
        "        X.append(series[i:i+seq_len])\n",
        "        Y.append(labels[i+seq_len-1])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "# ========== MODEL ==========\n",
        "def build_lstm_autoencoder(seq_len=SEQ, learning_rate=LEARNING_RATE):\n",
        "    inputs = tf.keras.Input(shape=(seq_len, 1))\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x); x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x); x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=False))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x); x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    bottleneck = tf.keras.layers.Dense(4, activation=\"linear\")(x)\n",
        "    x = tf.keras.layers.RepeatVector(seq_len)(bottleneck)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x); x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x); x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x); x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation=\"linear\"))(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss=\"huber\", metrics=[\"mse\"])\n",
        "    return model\n",
        "\n",
        "# ========== TUNING HELPERS ==========\n",
        "def sens_spec_for_tuning(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
        "    s  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
        "    sp = tn/(tn+fp) if (tn+fp)>0 else 0.0\n",
        "    return s, sp\n",
        "\n",
        "def tune_aggressive_threshold(y_val, anomaly_scores, spec_target=SPEC_TARGET, w_sens=TUNE_WEIGHT_SENS, w_spec=TUNE_WEIGHT_SPEC):\n",
        "    best_t, best_score = None, -1.0\n",
        "    for p in [85,87,89,90,91,92,93,94,95,96,97,98,98.5,99,99.2,99.5]:\n",
        "        t = np.percentile(anomaly_scores, p)\n",
        "        yhat = (anomaly_scores >= t).astype(int)\n",
        "        s, sp = sens_spec_for_tuning(y_val, yhat)\n",
        "        if sp >= spec_target:\n",
        "            score = w_sens*s + w_spec*sp\n",
        "            if score > best_score:\n",
        "                best_score, best_t = score, float(t)\n",
        "    if best_t is None:\n",
        "        for p in [80,82,84,85,86,87,88,89,90,91,92,93,94]:\n",
        "            t = np.percentile(anomaly_scores, p)\n",
        "            yhat = (anomaly_scores >= t).astype(int)\n",
        "            s, sp = sens_spec_for_tuning(y_val, yhat)\n",
        "            if sp >= spec_target:\n",
        "                score = w_sens*s + w_spec*sp\n",
        "                if score > best_score:\n",
        "                    best_score, best_t = score, float(t)\n",
        "    if best_t is None:\n",
        "        best_t = float(np.percentile(anomaly_scores, 80))\n",
        "        print(\"WARNING: No threshold achieved target specificity; using conservative 80th pct.\")\n",
        "    return best_t\n",
        "\n",
        "def tune_mix_and_threshold(Xval, Yval, recon_val,\n",
        "                           mixes=MIX_GRID, spec_target=SPEC_TARGET,\n",
        "                           w_sens=TUNE_WEIGHT_SENS, w_spec=TUNE_WEIGHT_SPEC):\n",
        "    val_mse = np.mean((Xval - recon_val)**2, axis=(1,2))\n",
        "    val_max = np.max(np.abs(Xval - recon_val), axis=(1,2))\n",
        "    best = dict(score=-1.0, mix=None, threshold=None, sens=None, spec=None)\n",
        "    for m_mse, m_max in mixes:\n",
        "        scores = m_mse*val_mse + m_max*val_max\n",
        "        thr = tune_aggressive_threshold(Yval, scores, spec_target, w_sens, w_spec)\n",
        "        yhat = (scores >= thr).astype(int)\n",
        "        s, sp = sens_spec_for_tuning(Yval, yhat)\n",
        "        if sp >= spec_target:\n",
        "            score = w_sens*s + w_spec*sp\n",
        "            if score > best[\"score\"]:\n",
        "                best.update(score=score, mix=(m_mse, m_max), threshold=thr, sens=s, spec=sp)\n",
        "    if best[\"mix\"] is None:\n",
        "        best_spec = -1.0\n",
        "        for m_mse, m_max in mixes:\n",
        "            scores = m_mse*val_mse + m_max*val_max\n",
        "            thr = float(np.percentile(scores, 80))\n",
        "            yhat = (scores >= thr).astype(int)\n",
        "            s, sp = sens_spec_for_tuning(Yval, yhat)\n",
        "            if sp > best_spec:\n",
        "                best_spec = sp\n",
        "                best.update(score=w_sens*s + w_spec*sp, mix=(m_mse, m_max),\n",
        "                            threshold=thr, sens=s, spec=sp)\n",
        "        print(\"NOTE: no mix hit the specificity target; chose the most specific fallback.\")\n",
        "    return best\n",
        "\n",
        "# ========== COMPARATOR METRICS (TAIL-ONLY) ==========\n",
        "def compute_fpr_tail(A_tail, O_tail):\n",
        "    FP = np.sum((A_tail == 1) & (O_tail == 0))\n",
        "    N0 = np.sum(O_tail == 0)\n",
        "    return (FP / N0) if N0 > 0 else np.nan\n",
        "\n",
        "def compute_specificity_tail(A_tail, O_tail):\n",
        "    TN = np.sum((A_tail == 0) & (O_tail == 0))\n",
        "    N0 = np.sum(O_tail == 0)\n",
        "    return (TN / N0) if N0 > 0 else np.nan\n",
        "\n",
        "def compute_sensitivity_tail(A_tail, O_tail):\n",
        "    TP = np.sum((A_tail == 1) & (O_tail > 0))\n",
        "    P  = np.sum(O_tail > 0)\n",
        "    return (TP / P) if P > 0 else np.nan\n",
        "\n",
        "def compute_pod_tail(A_tail, O_tail):\n",
        "    nsim = A_tail.shape[1]\n",
        "    hits = [np.any((A_tail[:, j] == 1) & (O_tail[:, j] > 0)) for j in range(nsim)]\n",
        "    return float(np.mean(hits)) if nsim > 0 else np.nan\n",
        "\n",
        "def compute_timeliness_tail(A_tail, O_tail):\n",
        "    nsim = A_tail.shape[1]\n",
        "    score = 0.0\n",
        "    miss  = 0\n",
        "    for j in range(nsim):\n",
        "        o = O_tail[:, j]\n",
        "        if np.sum(o > 0) == 0:\n",
        "            miss += 1\n",
        "            continue\n",
        "        r_idx = np.where(o > 0)[0]\n",
        "        r1, r2 = int(r_idx[0]), int(r_idx[-1])\n",
        "        a = A_tail[:, j]\n",
        "        hit_idx = np.where((a == 1) & (o > 0))[0]\n",
        "        if len(hit_idx) == 0:\n",
        "            miss += 1\n",
        "        else:\n",
        "            score += (int(hit_idx[0]) - r1) / (r2 - r1 + 1)\n",
        "    return (score + miss) / nsim if nsim > 0 else np.nan\n",
        "\n",
        "# ========== ES with min-epoch floor ==========\n",
        "class EarlyStoppingWithMin(tf.keras.callbacks.EarlyStopping):\n",
        "    def __init__(self, min_epochs=MIN_EPOCHS, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.min_epochs = min_epochs\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) < self.min_epochs:\n",
        "            return\n",
        "        return super().on_epoch_end(epoch, logs)\n",
        "\n",
        "# ========== MAIN ==========\n",
        "rng = np.random.RandomState(RNG_STATE)\n",
        "summary_rows = []\n",
        "\n",
        "# Load IF-driven splits once (supports explicit 60/20/20 if present)\n",
        "print(f\"Loading predefined IF splits:\\n  VAL:  {VAL_SPLIT_CSV}\\n  TEST: {TEST_SPLIT_CSV}\")\n",
        "by_signal = load_if_splits(VAL_SPLIT_CSV, TEST_SPLIT_CSV)\n",
        "\n",
        "for S in SIGNALS:\n",
        "    print(f\"\\n=== Signal {S} (LSTM AE; IF-driven splits; tail-only metrics) ===\")\n",
        "    Xdf, Ydf = load_signal(S)\n",
        "\n",
        "    # Build sim list from all columns\n",
        "    sims = []\n",
        "    for sim_idx, col in enumerate(Xdf.columns):\n",
        "        x = Xdf[col].to_numpy(dtype=float)\n",
        "        y = Ydf[col].to_numpy(dtype=int)\n",
        "        sim_name = f\"sig{S}_sim{sim_idx}\"\n",
        "        sims.append(dict(x=x, y=y, name=sim_name, col_idx=sim_idx))\n",
        "\n",
        "    # Pick train/val/test sims from the IF files (prefer explicit 'train' if provided)\n",
        "    sig_splits = by_signal.get(S, {\"train\": set(), \"val\": set(), \"test\": set()})\n",
        "    want_train = set(sig_splits.get(\"train\", set()))\n",
        "    want_val   = set(sig_splits.get(\"val\", set()))\n",
        "    want_test  = set(sig_splits.get(\"test\", set()))\n",
        "\n",
        "    train_sims, val_sims, test_sims = [], [], []\n",
        "    missing_train, missing_val, missing_test = set(), set(), set()\n",
        "    sim_by_name = {d[\"name\"]: d for d in sims}\n",
        "\n",
        "    use_explicit_train = len(want_train) > 0\n",
        "\n",
        "    if use_explicit_train:\n",
        "        for name in sorted(want_train):\n",
        "            d = sim_by_name.get(name)\n",
        "            if d is None: missing_train.add(name); continue\n",
        "            if len(d[\"x\"]) < (TRAIN_DAYS + TAIL_DAYS): continue\n",
        "            train_sims.append(d)\n",
        "\n",
        "    for name in sorted(want_val):\n",
        "        d = sim_by_name.get(name)\n",
        "        if d is None: missing_val.add(name); continue\n",
        "        if len(d[\"x\"]) < (TRAIN_DAYS + TAIL_DAYS): print(f\"  ⚠ Skipping VAL {name}: too short\"); continue\n",
        "        val_sims.append(d)\n",
        "\n",
        "    for name in sorted(want_test):\n",
        "        d = sim_by_name.get(name)\n",
        "        if d is None: missing_test.add(name); continue\n",
        "        if len(d[\"x\"]) < (TRAIN_DAYS + TAIL_DAYS): print(f\"  ⚠ Skipping TEST {name}: too short\"); continue\n",
        "        test_sims.append(d)\n",
        "\n",
        "    if not use_explicit_train:\n",
        "        picked = {d[\"name\"] for d in val_sims} | {d[\"name\"] for d in test_sims}\n",
        "        for d in sims:\n",
        "            if d[\"name\"] in picked: continue\n",
        "            if len(d[\"x\"]) >= (TRAIN_DAYS + TAIL_DAYS): train_sims.append(d)\n",
        "\n",
        "    print(f\"  Using splits (from IF files): {len(train_sims)} train, {len(val_sims)} val, {len(test_sims)} test\")\n",
        "    if missing_train: print(f\"  ⚠ Missing in data (TRAIN): {sorted(missing_train)[:5]}{' ...' if len(missing_train)>5 else ''}\")\n",
        "    if missing_val:  print(f\"  ⚠ Missing in data (VAL):   {sorted(missing_val)[:5]}{' ...' if len(missing_val)>5 else ''}\")\n",
        "    if missing_test: print(f\"  ⚠ Missing in data (TEST):  {sorted(missing_test)[:5]}{' ...' if len(missing_test)>5 else ''}\")\n",
        "\n",
        "    # Optional confirmation dump\n",
        "    pd.Series([d[\"col_idx\"] for d in val_sims]).to_csv(f\"lstm_val_indices_sig{S}.csv\", index=False, header=False)\n",
        "    pd.Series([d[\"col_idx\"] for d in test_sims]).to_csv(f\"lstm_test_indices_sig{S}.csv\", index=False, header=False)\n",
        "\n",
        "    if not train_sims:\n",
        "        print(\"  No training sims with enough length; skipping.\"); continue\n",
        "    if not test_sims:\n",
        "        print(\"  No test sims with enough length; skipping.\"); continue\n",
        "\n",
        "    # ---- Unsupervised TRAIN windows from first 6y\n",
        "    Xtr_list = []\n",
        "    for d in train_sims:\n",
        "        x_train = d[\"x\"][:TRAIN_DAYS]\n",
        "        X_seq, _ = make_seq_labels(x_train, np.zeros_like(x_train), SEQ, STRIDE)\n",
        "        if len(X_seq): Xtr_list.append(X_seq)\n",
        "    if not Xtr_list:\n",
        "        print(\"  No training windows; skipping.\"); continue\n",
        "    Xtr = np.concatenate(Xtr_list).reshape(-1, SEQ, 1).astype(np.float32)\n",
        "    print(f\"  Train windows: {len(Xtr)} from first {TRAIN_YEARS} years\")\n",
        "\n",
        "    # ---- VALIDATION: build with (SEQ-1) days of CONTEXT so we get 1 score per tail day (no padding)\n",
        "    Xv_list, Yv_list = [], []\n",
        "    for d in val_sims:\n",
        "        x = d[\"x\"]; y = d[\"y\"]\n",
        "        tail_start = len(x) - TAIL_DAYS\n",
        "        ctx_start  = tail_start - (SEQ - 1)\n",
        "        if ctx_start < 0: continue\n",
        "        x_ctx_tail = x[ctx_start : tail_start + TAIL_DAYS]\n",
        "        y_ctx_tail = y[ctx_start : tail_start + TAIL_DAYS]\n",
        "        Xv, Yv = make_seq_labels(x_ctx_tail, y_ctx_tail, SEQ, STRIDE)  # len == TAIL_DAYS\n",
        "        if len(Xv) == TAIL_DAYS:\n",
        "            Xv_list.append(Xv); Yv_list.append(Yv)\n",
        "    Xval = np.concatenate(Xv_list).reshape(-1, SEQ, 1).astype(np.float32) if Xv_list else np.empty((0,SEQ,1), np.float32)\n",
        "    Yval = np.concatenate(Yv_list).astype(int) if Yv_list else np.empty((0,), np.int32)\n",
        "    if len(Yval):\n",
        "        pos = int(Yval.sum()); pct = 100*Yval.mean()\n",
        "        print(f\"  Val windows (tail, full coverage): {len(Xval)}  positives: {pos} ({pct:.1f}%)\")\n",
        "    else:\n",
        "        print(\"  No val windows (tuner will fall back to train guard-rail).\")\n",
        "\n",
        "    # ---- TEST: same context trick → exactly TAIL_DAYS scores per sim, no front padding\n",
        "    per_sim_labels = []   # each length = TAIL_DAYS\n",
        "    A_tail_cols    = []   # alarms per sim (343)\n",
        "    O_tail_cols    = []   # labels per sim (343)\n",
        "    Xte_chunks     = []   # each chunk length = TAIL_DAYS (of SEQ-length windows)\n",
        "\n",
        "    for d in test_sims:\n",
        "        x = d[\"x\"]; y = d[\"y\"]\n",
        "        tail_start = len(x) - TAIL_DAYS\n",
        "        ctx_start  = tail_start - (SEQ - 1)\n",
        "        if ctx_start < 0: continue\n",
        "        x_ctx_tail = x[ctx_start : tail_start + TAIL_DAYS]\n",
        "        y_tail     = y[tail_start : tail_start + TAIL_DAYS].astype(int)  # 343 labels aligned to tail days\n",
        "        Xte, _     = make_seq_labels(x_ctx_tail, y[ctx_start : tail_start + TAIL_DAYS], SEQ, STRIDE)  # len == 343\n",
        "        if len(Xte) == TAIL_DAYS:\n",
        "            Xte_chunks.append(Xte)\n",
        "            per_sim_labels.append(y_tail)\n",
        "\n",
        "    if not Xte_chunks:\n",
        "        print(\"  No test windows; skipping.\"); continue\n",
        "\n",
        "    Xte = np.concatenate(Xte_chunks).reshape(-1, SEQ, 1).astype(np.float32)\n",
        "    print(f\"  Test windows (tail, full coverage): {len(Xte)}\")\n",
        "\n",
        "    # ---- Build & train\n",
        "    model = build_lstm_autoencoder(SEQ, LEARNING_RATE)\n",
        "    if len(Xval):\n",
        "        cbs = [\n",
        "            EarlyStoppingWithMin(min_epochs=MIN_EPOCHS, patience=PATIENCE, restore_best_weights=True, monitor=\"val_loss\"),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6),\n",
        "            tf.keras.callbacks.TerminateOnNaN(),\n",
        "        ]\n",
        "        model.fit(Xtr, Xtr, validation_data=(Xval, Xval), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=cbs, verbose=1)\n",
        "    else:\n",
        "        cbs = [\n",
        "            EarlyStoppingWithMin(min_epochs=MIN_EPOCHS, patience=PATIENCE, restore_best_weights=True, monitor=\"loss\"),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=5, min_lr=1e-6),\n",
        "            tf.keras.callbacks.TerminateOnNaN(),\n",
        "        ]\n",
        "        model.fit(Xtr, Xtr, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=cbs, verbose=1)\n",
        "\n",
        "    # ---- Validation: tune mix + threshold (or fallback if no val)\n",
        "    if len(Xval):\n",
        "        val_rec = model.predict(Xval, batch_size=BATCH_SIZE, verbose=0)\n",
        "        best = tune_mix_and_threshold(\n",
        "            Xval, Yval, val_rec,\n",
        "            mixes=MIX_GRID, spec_target=SPEC_TARGET,\n",
        "            w_sens=TUNE_WEIGHT_SENS, w_spec=TUNE_WEIGHT_SPEC\n",
        "        )\n",
        "        AM_MSE, AM_MAX = best[\"mix\"]\n",
        "        best_threshold = best[\"threshold\"]\n",
        "\n",
        "        # Guard-rail on TRAIN with chosen mix\n",
        "        tr_rec_for_guard = model.predict(Xtr, batch_size=BATCH_SIZE, verbose=0)\n",
        "        tr_mse_g = np.mean((Xtr - tr_rec_for_guard)**2, axis=(1,2))\n",
        "        tr_max_g = np.max(np.abs(Xtr - tr_rec_for_guard), axis=(1,2))\n",
        "        tr_scores_g = AM_MSE*tr_mse_g + AM_MAX*tr_max_g\n",
        "        guardrail_thr = float(np.percentile(tr_scores_g, 95))\n",
        "        if best_threshold < guardrail_thr:\n",
        "            print(f\"  Guard-rail raised threshold: {best_threshold:.6f} -> {guardrail_thr:.6f}\")\n",
        "            best_threshold = guardrail_thr\n",
        "\n",
        "        print(f\"  Chosen mix: MSE={AM_MSE:.2f}, MAX={AM_MAX:.2f} | \"\n",
        "              f\"threshold={best_threshold:.6f} | val Sens={best['sens']:.3f}, Spec={best['spec']:.3f}\")\n",
        "    else:\n",
        "        tr_rec = model.predict(Xtr, batch_size=BATCH_SIZE, verbose=0)\n",
        "        tr_mse = np.mean((Xtr - tr_rec)**2, axis=(1,2))\n",
        "        best_threshold = float(np.percentile(tr_mse, 95))\n",
        "        AM_MSE, AM_MAX = ANOMALY_MIX_MSE, ANOMALY_MIX_MAX\n",
        "        print(f\"  No validation; using default mix MSE={AM_MSE:.2f}, MAX={AM_MAX:.2f} and 95th pct train MSE threshold.\")\n",
        "\n",
        "    # ---- TEST inference (one score per tail day per sim; no padding)\n",
        "    te_rec  = model.predict(Xte, batch_size=BATCH_SIZE, verbose=0)\n",
        "    te_mse  = np.mean((Xte - te_rec)**2, axis=(1,2))\n",
        "    te_max  = np.max(np.abs(Xte - te_rec), axis=(1,2))\n",
        "    te_scores = AM_MSE*te_mse + AM_MAX*te_max  # length = TAIL_DAYS * n_test_sims\n",
        "\n",
        "    # Chunk back per sim (each chunk length = TAIL_DAYS)\n",
        "    ofs = 0\n",
        "    for y_tail in per_sim_labels:\n",
        "        yhat_seq = (te_scores[ofs:ofs+TAIL_DAYS] >= best_threshold).astype(int)\n",
        "        ofs += TAIL_DAYS\n",
        "        A_tail_cols.append(yhat_seq)           # length 343\n",
        "        O_tail_cols.append(y_tail.astype(int)) # length 343\n",
        "\n",
        "    if not A_tail_cols:\n",
        "        print(\"  No test sims; skipping metrics.\"); continue\n",
        "\n",
        "    A_tail = np.column_stack(A_tail_cols)  # [343, nsim_test]\n",
        "    O_tail = np.column_stack(O_tail_cols)  # [343, nsim_test]\n",
        "\n",
        "    # ---- Tail-only metrics\n",
        "    sens = compute_sensitivity_tail(A_tail, O_tail)\n",
        "    spec = compute_specificity_tail(A_tail, O_tail)\n",
        "    fpr  = compute_fpr_tail(A_tail, O_tail)\n",
        "    pod  = compute_pod_tail(A_tail, O_tail)\n",
        "    tim  = compute_timeliness_tail(A_tail, O_tail)\n",
        "\n",
        "    print(f\"  TAIL-ONLY → Sens={sens:.3f}, Spec={spec:.3f}, FPR={fpr:.3f}, POD={pod:.3f}, Tim={tim:.3f}\")\n",
        "\n",
        "    summary_rows.append(dict(\n",
        "        signal=S, sensitivity=sens, specificity=spec, fpr=fpr, pod=pod, timeliness=tim,\n",
        "        threshold=best_threshold, mix_mse=AM_MSE, mix_max=AM_MAX, nsim_test=A_tail.shape[1]\n",
        "    ))\n",
        "\n",
        "# ========== SAVE SUMMARY ==========\n",
        "if summary_rows:\n",
        "    df = pd.DataFrame(summary_rows).set_index(\"signal\").sort_index()\n",
        "    print(\"\\n=== LSTM Autoencoder — Tail-only Comparator Metrics (by signal) ===\")\n",
        "    print(df.round(6))\n",
        "    print(\"\\n=== Means ===\")\n",
        "    print(df[[\"sensitivity\",\"specificity\",\"fpr\",\"pod\",\"timeliness\"]].mean().round(6))\n",
        "    df.to_csv(\"LSTM_AE_tail_only_metrics.csv\")\n",
        "    print(\"\\nSaved: LSTM_AE_tail_only_metrics.csv\")\n",
        "else:\n",
        "    print(\"\\nNo results to summarize.\")\n"
      ]
    }
  ]
}