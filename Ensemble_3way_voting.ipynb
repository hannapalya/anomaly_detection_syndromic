{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hannapalya/anomaly_detection_syndromic/blob/main/Ensemble_3way_voting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Way Voting Ensemble: LSTM-AE + Isolation Forest + One-Class SVM\n",
    "\n",
    "## Overview\n",
    "Ensemble method combining three anomaly detection algorithms using majority voting.\n",
    "\n",
    "**Key Features:**\n",
    "- Each model gets its own tuned threshold on ALIGNED validation data\n",
    "- Alignment is done from the END to ensure all models cover the same time period\n",
    "- Final prediction: At least 2 of 3 models must agree (majority voting)\n",
    "\n",
    "**Outputs:**\n",
    "- `Ensemble_3way_voting_results.csv` - Summary results\n",
    "- `Ensemble_3way_voting_per_sim_val.csv` - Validation metrics per simulation\n",
    "- `Ensemble_3way_voting_per_sim_test.csv` - Test metrics per simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cw9mZ7RJyufc"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "3-Way Voting Ensemble: LSTM-AE + Isolation Forest + One-Class SVM (FIXED - END ALIGNMENT)\n",
    "\n",
    "Voting Logic:\n",
    "  - Each model gets its own tuned threshold on ALIGNED validation data to achieve >= SPEC_TARGET\n",
    "  - Alignment is done from the END to ensure all models cover the same time period\n",
    "  - Final prediction: At least 2 of 3 models must agree (majority voting)\n",
    "\n",
    "Outputs:\n",
    "  - Ensemble_3way_voting_per_sim_val.csv\n",
    "  - Ensemble_3way_voting_per_sim_test.csv\n",
    "  - Ensemble_3way_voting_results.csv\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ===== USER CONFIG =====\n",
    "DATA_DIR      = \"/content\"\n",
    "SIGNALS       = list(range(1, 17))\n",
    "DAYS_PER_YEAR = 364\n",
    "TRAIN_YEARS   = 6\n",
    "TRAIN_DAYS    = TRAIN_YEARS * DAYS_PER_YEAR\n",
    "VALID_DAYS    = 49 * 7\n",
    "RNG_STATE     = 42\n",
    "\n",
    "# Threshold tuning\n",
    "SPEC_TARGET = 0.95\n",
    "SENS_FLOOR  = 0.00\n",
    "BETA        = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== IMPORT ADAPTERS =====\n",
    "import importlib.util\n",
    "import importlib\n",
    "import pathlib\n",
    "\n",
    "def import_adapter(path_str: str, func_name: str, module_name: str):\n",
    "    p = pathlib.Path(path_str)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Adapter not found at: {p}\")\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "    spec = importlib.util.spec_from_file_location(module_name, str(p))\n",
    "    mod = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(mod)\n",
    "    if not hasattr(mod, func_name):\n",
    "        raise AttributeError(f\"{p.name} is missing `{func_name}()`\")\n",
    "    return getattr(mod, func_name)\n",
    "\n",
    "lstm_fit_and_score = import_adapter(\"LSTM_AE_curr.py\", \"fit_and_score\", module_name=\"adapter_lstm\")\n",
    "iso_fit_and_score  = import_adapter(\"IsolationForest_tuned.py\", \"fit_and_score\", module_name=\"adapter_if\")\n",
    "ocsvm_fit_and_score = import_adapter(\"OneClassSVM_adapter.py\", \"fit_and_score\", module_name=\"adapter_ocsvm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== HELPERS =====\n",
    "def load_data(sig: int):\n",
    "    X = pd.read_csv(os.path.join(DATA_DIR, f\"simulated_totals_sig{sig}.csv\"))\n",
    "    Y = (pd.read_csv(os.path.join(DATA_DIR, f\"simulated_outbreaks_sig{sig}.csv\")) > 0).astype(int)\n",
    "    date_col = next((c for c in [\"date\",\"Date\",\"ds\",\"timestamp\"] if c in X.columns), None)\n",
    "    if date_col:\n",
    "        X = X.drop(columns=[date_col])\n",
    "        if date_col in Y.columns: Y = Y.drop(columns=[date_col])\n",
    "    return X, Y\n",
    "\n",
    "def cross_sim_split(sims: List[dict], rng: np.random.RandomState, train_frac=0.6):\n",
    "    rng.shuffle(sims)\n",
    "    n_train = int(len(sims) * train_frac)\n",
    "    return sims[:n_train], sims[n_train:]\n",
    "\n",
    "def sens_spec(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float,float]:\n",
    "    if len(y_true) == 0:\n",
    "        return np.nan, np.nan\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    sens = TP/(TP+FN) if (TP+FN)>0 else np.nan\n",
    "    spec = TN/(TN+FP) if (TN+FP)>0 else np.nan\n",
    "    return sens, spec\n",
    "\n",
    "def tune_threshold(y_val: np.ndarray, scores: np.ndarray,\n",
    "                   spec_target=SPEC_TARGET, sens_floor=SENS_FLOOR, p_max=20.0) -> float:\n",
    "    if len(scores) == 0:\n",
    "        return float(\"inf\")\n",
    "    neg_mask = (y_val == 0)\n",
    "    base = scores[neg_mask] if neg_mask.any() else scores\n",
    "    grid = np.linspace(0.5, p_max, num=200)\n",
    "    best_t = None\n",
    "    best_tuple = None\n",
    "    for p in grid:\n",
    "        thr = np.percentile(base, 100 - p)\n",
    "        yhat = (scores >= thr).astype(int)\n",
    "        s, sp = sens_spec(y_val, yhat)\n",
    "        if (sp is not None) and (s is not None) and (sp >= spec_target) and (s >= sens_floor):\n",
    "            key = (s, sp)\n",
    "            if (best_tuple is None) or (key > best_tuple):\n",
    "                best_tuple = key\n",
    "                best_t = float(thr)\n",
    "    if best_t is None:\n",
    "        best_gap = 1e9\n",
    "        best_s=-1.0\n",
    "        best_sp=-1.0\n",
    "        for p in grid:\n",
    "            thr = np.percentile(base, 100 - p)\n",
    "            yhat = (scores >= thr).astype(int)\n",
    "            s, sp = sens_spec(y_val, yhat)\n",
    "            gap = abs(sp - spec_target) if sp is not None else 1e9\n",
    "            if (gap < best_gap) or (gap == best_gap and (sp > best_sp or (sp == best_sp and s > best_s))):\n",
    "                best_t = float(thr)\n",
    "                best_gap = gap\n",
    "                best_sp = (sp if sp is not None else -1)\n",
    "                best_s = (s if s is not None else -1)\n",
    "    return best_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional metric functions\n",
    "def metric_pod_anyhit(yhat: np.ndarray, ytrue: np.ndarray) -> float:\n",
    "    has_out = (ytrue == 1).any()\n",
    "    if not has_out: return np.nan\n",
    "    return 1.0 if ((yhat == 1) & (ytrue == 1)).any() else 0.0\n",
    "\n",
    "def timeliness_single(yhat: np.ndarray, ytrue: np.ndarray) -> float:\n",
    "    idx_out = np.where(ytrue > 0)[0]\n",
    "    if len(idx_out) == 0: return 1.0\n",
    "    idx_hit = np.where((ytrue > 0) & (yhat > 0))[0]\n",
    "    if len(idx_hit) == 0: return 1.0\n",
    "    r1, r2 = int(idx_out[0]), int(idx_out[-1])\n",
    "    obs = int(idx_hit[0])\n",
    "    return (obs - r1) / (r2 - r1 + 1)\n",
    "\n",
    "def split_by_lengths(arr: np.ndarray, lengths: List[int]) -> List[np.ndarray]:\n",
    "    out = []\n",
    "    i = 0\n",
    "    total = len(arr)\n",
    "    for L in lengths:\n",
    "        if i >= total:\n",
    "            out.append(arr[:0])\n",
    "            continue\n",
    "        out.append(arr[i:min(i+L, total)])\n",
    "        i += L\n",
    "    return out\n",
    "\n",
    "def val_lengths_for_window(val_sims: List[dict], window: int) -> List[int]:\n",
    "    Ls = []\n",
    "    for d in val_sims:\n",
    "        y_tail = d[\"y\"][-VALID_DAYS:]\n",
    "        Ls.append(max(0, len(y_tail) - (window - 1)))\n",
    "    return Ls\n",
    "\n",
    "def _align_tail(O, T):\n",
    "    return O[-T:] if len(O) >= T else np.pad(O, (T-len(O), 0))\n",
    "\n",
    "def metric_sensitivity(A, O):\n",
    "    Oa = _align_tail(O, A.shape[0])\n",
    "    TP = np.logical_and(A==1, Oa>0).sum()\n",
    "    FN = np.logical_and(A==0, Oa>0).sum()\n",
    "    return (TP/(TP+FN)) if (TP+FN)>0 else np.nan\n",
    "\n",
    "def metric_specificity(A, O):\n",
    "    Oa = _align_tail(O, A.shape[0])\n",
    "    TN = np.logical_and(A==0, Oa==0).sum()\n",
    "    FP = np.logical_and(A==1, Oa==0).sum()\n",
    "    return (TN/(TN+FP)) if (TN+FP)>0 else np.nan\n",
    "\n",
    "def metric_pod(A, O):\n",
    "    Oa = _align_tail(O, A.shape[0])\n",
    "    return np.mean((np.logical_and(A==1, Oa>0)).sum(axis=0) > 0)\n",
    "\n",
    "def metric_timeliness(A, O):\n",
    "    Oa = _align_tail(O, A.shape[0])\n",
    "    T, J = A.shape\n",
    "    score=0.0\n",
    "    for j in range(J):\n",
    "        score += timeliness_single(A[:,j], Oa[:,j])\n",
    "    return score / J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MAIN =====\n",
    "np.random.seed(RNG_STATE)\n",
    "rng = np.random.RandomState(RNG_STATE)\n",
    "\n",
    "summary = {}\n",
    "rows_val = []\n",
    "rows_test = []\n",
    "\n",
    "for S in SIGNALS:\n",
    "    print(f\"\\n--- Signal {S} (3-Way Voting: LSTM + IF + OCSVM) ---\")\n",
    "    Xsig, Ysig = load_data(S)\n",
    "\n",
    "    # Build sim dicts\n",
    "    sims = []\n",
    "    for sim_idx, col in enumerate(Xsig.columns):\n",
    "        x = Xsig[col].to_numpy(np.float32, copy=False)\n",
    "        y = Ysig[col].to_numpy(np.int32, copy=False)\n",
    "        if len(x) >= TRAIN_DAYS + VALID_DAYS:\n",
    "            sims.append(dict(sim=f\"sig{S}_sim{sim_idx}\", x=x, y=y))\n",
    "    if not sims:\n",
    "        print(\"  No complete sims; skip.\")\n",
    "        continue\n",
    "\n",
    "    train_sims, held_sims = cross_sim_split(sims, rng, train_frac=0.6)\n",
    "    mid = max(1, len(held_sims)//2)\n",
    "    val_sims  = held_sims[:mid]\n",
    "    test_sims = held_sims[mid:] if len(held_sims) > 1 else held_sims\n",
    "    print(f\"  Using {len(train_sims)} train, {len(val_sims)} val, {len(test_sims)} test sims\")\n",
    "\n",
    "    # === Get results from all 3 models ===\n",
    "    print(\"  Running LSTM-AE...\")\n",
    "    lstm_res = lstm_fit_and_score(S, train_sims, val_sims, test_sims, rng_state=RNG_STATE)\n",
    "    print(\"  Running Isolation Forest...\")\n",
    "    iso_res = iso_fit_and_score(S, train_sims, val_sims, test_sims, rng_state=RNG_STATE)\n",
    "    print(\"  Running One-Class SVM...\")\n",
    "    ocsvm_res = ocsvm_fit_and_score(S, train_sims, val_sims, test_sims, rng_state=RNG_STATE)\n",
    "\n",
    "    models = [\n",
    "        (\"LSTM-AE\", lstm_res),\n",
    "        (\"IF\", iso_res),\n",
    "        (\"OCSVM\", ocsvm_res)\n",
    "    ]\n",
    "\n",
    "    # === STEP 1: Collect all predictions (full length) ===\n",
    "    model_full_preds = {}\n",
    "    for name, res in models:\n",
    "        yv_concat = res[\"val_labels\"].astype(np.int32)\n",
    "        sv_concat = res[\"val_scores\"].astype(np.float32)\n",
    "        model_full_preds[name] = {\n",
    "            \"scores\": sv_concat,\n",
    "            \"labels\": yv_concat,\n",
    "            \"window\": res[\"window\"],\n",
    "            \"res\": res\n",
    "        }\n",
    "        print(f\"    {name}: window={res['window']}, val_length={len(sv_concat)}\")\n",
    "\n",
    "    # === STEP 2: Find minimum length for alignment ===\n",
    "    min_len = min(len(model_full_preds[name][\"scores\"]) for name in model_full_preds.keys())\n",
    "    print(f\"  Aligning all models to min_len={min_len} (from END)\")\n",
    "\n",
    "    # === STEP 3: Align all predictions to minimum length (FROM THE END) ===\n",
    "    for name in model_full_preds.keys():\n",
    "        # CRITICAL FIX: Take the LAST min_len points so all models cover the same time period\n",
    "        model_full_preds[name][\"scores_aligned\"] = model_full_preds[name][\"scores\"][-min_len:]\n",
    "        model_full_preds[name][\"labels_aligned\"] = model_full_preds[name][\"labels\"][-min_len:]\n",
    "\n",
    "    # === STEP 4: Tune thresholds on ALIGNED data ===\n",
    "    thresholds = {}\n",
    "    val_preds = {}\n",
    "\n",
    "    for name in model_full_preds.keys():\n",
    "        sv_aligned = model_full_preds[name][\"scores_aligned\"]\n",
    "        yv_aligned = model_full_preds[name][\"labels_aligned\"]\n",
    "\n",
    "        thr = tune_threshold(yv_aligned, sv_aligned, SPEC_TARGET, SENS_FLOOR)\n",
    "        yhat = (sv_aligned >= thr).astype(int)\n",
    "        s, sp = sens_spec(yv_aligned, yhat)\n",
    "\n",
    "        thresholds[name] = thr\n",
    "        val_preds[name] = {\n",
    "            \"scores\": sv_aligned,  # Use aligned\n",
    "            \"preds\": yhat,\n",
    "            \"labels\": yv_aligned,  # Use aligned\n",
    "            \"window\": model_full_preds[name][\"window\"],\n",
    "            \"res\": model_full_preds[name][\"res\"]\n",
    "        }\n",
    "\n",
    "        print(f\"    {name}: thr={thr:.6f}, val_sens={s:.3f}, val_spec={sp:.3f}\")\n",
    "\n",
    "    # === STEP 5: Apply 3-way voting on validation (now properly aligned) ===\n",
    "    lstm_pred = val_preds[\"LSTM-AE\"][\"preds\"]\n",
    "    if_pred = val_preds[\"IF\"][\"preds\"]\n",
    "    ocsvm_pred = val_preds[\"OCSVM\"][\"preds\"]\n",
    "    val_labels = val_preds[\"LSTM-AE\"][\"labels\"]  # All same length now\n",
    "\n",
    "    # Voting: sum >= 2 means at least 2 models detected\n",
    "    vote_sum = lstm_pred + if_pred + ocsvm_pred\n",
    "    ensemble_pred = (vote_sum >= 2).astype(int)\n",
    "\n",
    "    # Compute validation metrics\n",
    "    val_sens, val_spec = sens_spec(val_labels, ensemble_pred)\n",
    "\n",
    "    # Per-sim validation metrics (use most common window or max)\n",
    "    window_used = max(val_preds[\"LSTM-AE\"][\"window\"],\n",
    "                     val_preds[\"IF\"][\"window\"],\n",
    "                     val_preds[\"OCSVM\"][\"window\"])\n",
    "    Ls_val = val_lengths_for_window(val_sims, window_used)\n",
    "\n",
    "    # Split ensemble predictions by sim\n",
    "    ens_splits = split_by_lengths(ensemble_pred, Ls_val)\n",
    "    lab_splits = split_by_lengths(val_labels, Ls_val)\n",
    "\n",
    "    val_pods = []\n",
    "    for sim_d, yh, yv in zip(val_sims, ens_splits, lab_splits):\n",
    "        if len(yh) != len(yv):\n",
    "            L = min(len(yh), len(yv))\n",
    "            yh, yv = yh[:L], yv[:L]\n",
    "        s_i, sp_i = sens_spec(yv, yh) if len(yh) else (np.nan, np.nan)\n",
    "        pod_i = metric_pod_anyhit(yh, yv) if len(yh) else np.nan\n",
    "        tim_i = timeliness_single(yh, yv) if len(yh) else np.nan\n",
    "        val_pods.append(pod_i)\n",
    "\n",
    "        rows_val.append(dict(\n",
    "            split=\"val\", signal=S, sim=sim_d[\"sim\"], model=\"3-Way-Voting\",\n",
    "            window=int(window_used), sens=s_i, spec=sp_i, pod=pod_i,\n",
    "            timeliness=tim_i, n_points=int(len(yh))\n",
    "        ))\n",
    "\n",
    "    val_pod = float(np.nanmean(val_pods)) if val_pods else 0.0\n",
    "    print(f\"  VALIDATION → Voting sens={val_sens:.3f}, spec={val_spec:.3f}, POD={val_pod:.3f}\")\n",
    "\n",
    "    # === Apply 3-way voting on test ===\n",
    "    # Get test predictions from each model\n",
    "    lstm_test = val_preds[\"LSTM-AE\"][\"res\"]\n",
    "    if_test = val_preds[\"IF\"][\"res\"]\n",
    "    ocsvm_test = val_preds[\"OCSVM\"][\"res\"]\n",
    "\n",
    "    # Build test predictions for each model\n",
    "    test_preds_ensemble = []\n",
    "    test_labels_list = []\n",
    "\n",
    "    for idx, sim_d in enumerate(test_sims):\n",
    "        y_tail = sim_d[\"y\"][-VALID_DAYS:]\n",
    "\n",
    "        # LSTM\n",
    "        lstm_scores = lstm_test[\"test_scores_splits\"][idx]\n",
    "        lstm_win = lstm_test[\"window\"]\n",
    "        lstm_labels = y_tail[lstm_win-1:]\n",
    "        lstm_yh = (lstm_scores >= thresholds[\"LSTM-AE\"]).astype(int)\n",
    "\n",
    "        # IF\n",
    "        if_scores = if_test[\"test_scores_splits\"][idx]\n",
    "        if_win = if_test[\"window\"]\n",
    "        if_labels = y_tail[if_win-1:]\n",
    "        if_yh = (if_scores >= thresholds[\"IF\"]).astype(int)\n",
    "\n",
    "        # OCSVM\n",
    "        ocsvm_scores = ocsvm_test[\"test_scores_splits\"][idx]\n",
    "        ocsvm_win = ocsvm_test[\"window\"]\n",
    "        ocsvm_labels = y_tail[ocsvm_win-1:]\n",
    "        ocsvm_yh = (ocsvm_scores >= thresholds[\"OCSVM\"]).astype(int)\n",
    "\n",
    "        # Align to minimum length (from END for test too)\n",
    "        min_test_len = min(len(lstm_yh), len(if_yh), len(ocsvm_yh))\n",
    "        if min_test_len == 0:\n",
    "            continue\n",
    "\n",
    "        lstm_yh = lstm_yh[-min_test_len:]\n",
    "        if_yh = if_yh[-min_test_len:]\n",
    "        ocsvm_yh = ocsvm_yh[-min_test_len:]\n",
    "        test_labels = lstm_labels[-min_test_len:]  # Also from end\n",
    "\n",
    "        # Voting\n",
    "        vote_sum_test = lstm_yh + if_yh + ocsvm_yh\n",
    "        ens_yh = (vote_sum_test >= 2).astype(int)\n",
    "\n",
    "        test_preds_ensemble.append(ens_yh)\n",
    "        test_labels_list.append(test_labels)\n",
    "\n",
    "        # Per-sim test metrics\n",
    "        s_i, sp_i = sens_spec(test_labels, ens_yh)\n",
    "        pod_i = metric_pod_anyhit(ens_yh, test_labels)\n",
    "        tim_i = timeliness_single(ens_yh, test_labels)\n",
    "\n",
    "        rows_test.append(dict(\n",
    "            split=\"test\", signal=S, sim=sim_d[\"sim\"], model=\"3-Way-Voting\",\n",
    "            window=int(window_used), sens=s_i, spec=sp_i, pod=pod_i,\n",
    "            timeliness=tim_i, n_points=int(min_test_len)\n",
    "        ))\n",
    "\n",
    "    # Aggregate test metrics\n",
    "    if test_preds_ensemble:\n",
    "        min_test_len_agg = min(len(p) for p in test_preds_ensemble)\n",
    "        A = np.stack([p[:min_test_len_agg] for p in test_preds_ensemble], axis=1)\n",
    "        O = np.stack([l[:min_test_len_agg] for l in test_labels_list], axis=1)\n",
    "\n",
    "        test_sens = metric_sensitivity(A, O)\n",
    "        test_spec = metric_specificity(A, O)\n",
    "        test_pod = metric_pod(A, O)\n",
    "        test_tim = metric_timeliness(A, O)\n",
    "\n",
    "        print(f\"  TEST → Voting sens={test_sens:.3f}, spec={test_spec:.3f}, POD={test_pod:.3f}, tim={test_tim:.3f}\")\n",
    "\n",
    "        summary[S] = dict(\n",
    "            model=\"3-Way-Voting\",\n",
    "            window=int(window_used),\n",
    "            val_sens=float(val_sens), val_spec=float(val_spec), val_pod=float(val_pod),\n",
    "            sensitivity=float(test_sens), specificity=float(test_spec),\n",
    "            pod=float(test_pod), timeliness=float(test_tim)\n",
    "        )\n",
    "\n",
    "# ===== SAVE RESULTS =====\n",
    "if summary:\n",
    "    df = pd.DataFrame.from_dict(summary, orient=\"index\")\n",
    "    print(\"\\n=== 3-WAY VOTING ENSEMBLE SUMMARY ===\")\n",
    "    print(df)\n",
    "    print(\"\\nMeans:\\n\", df[[\"sensitivity\",\"specificity\",\"pod\",\"timeliness\"]].mean(numeric_only=True))\n",
    "    df.to_csv(\"Ensemble_3way_voting_results.csv\", index=True)\n",
    "    print(\"Saved: Ensemble_3way_voting_results.csv\")\n",
    "\n",
    "if rows_val:\n",
    "    dfv = pd.DataFrame(rows_val)\n",
    "    dfv.sort_values([\"signal\",\"sim\"], inplace=True)\n",
    "    dfv.to_csv(\"Ensemble_3way_voting_per_sim_val.csv\", index=False)\n",
    "    print(\"Saved: Ensemble_3way_voting_per_sim_val.csv\")\n",
    "\n",
    "if rows_test:\n",
    "    dft = pd.DataFrame(rows_test)\n",
    "    dft.sort_values([\"signal\",\"sim\"], inplace=True)\n",
    "    dft.to_csv(\"Ensemble_3way_voting_per_sim_test.csv\", index=False)\n",
    "    print(\"Saved: Ensemble_3way_voting_per_sim_test.csv\")\n",
    "\n",
    "print(\"\\n=== 3-WAY VOTING ENSEMBLE COMPLETE ===\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
